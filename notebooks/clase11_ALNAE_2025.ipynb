{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkE21DY+sUXSCY6tLYXQwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase11_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 11 (Viernes 25 de abril, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "X4LxHqF6tmVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descomposición en Valores Singulares\n",
        "\n",
        "El teorema espectral nos dice que bajo ciertas condiciones una matriz cuadrada puede escribirse como $A=XDX ^{-1}$, con $X$ invertible y $D$ diagonal.\n",
        "\n",
        "En el caso de que $A$ sea simétrica, tenemos $A=QDQ^T$, con $Q$ ortogonal.\n",
        "\n",
        "Ahora, qué ocurre si nuestra matriz $A$ no es cuadrada? Es claro que la noción de vectores propios no tiene sentido, dado que al aplicar $A$ sobre un vector, el resultado es un nuevo vector de dimensiones diferentes.\n",
        "\n",
        "Un resultado fundamental, que es la descomposición más importante en aplicaciones de álgebra lineal (y sobre todo en la Ciencia de Datos), es la siguiente:\n",
        "\n",
        "---\n",
        "\n",
        "**Descomposición SVD**\n",
        "\n",
        "Sea $A$ una matriz $m\\times n$ (i.e. $A:\\mathbb{R}^n\\to\\mathbb{R}^m$.\n",
        "Entonces existen\n",
        "- base ortonormal  $\\{v_1,\\ldots,v_n\\}$ de $\\mathbb{R}^n$,\n",
        "- baser ortonormal $\\{u_1,\\ldots,u_m\\}$ de $\\mathbb{R}^m$\n",
        "- números positivos $\\sigma_1\\geq\\sigma_2\\geq\\ldots,\\sigma_r>0$, siendo $r$ el rango de $A$\n",
        "\n",
        "tales que\n",
        "$$\n",
        "Av_1=\\sigma_1u_1,\\; \\ldots\\;, Av_r=\\sigma_ru_r,\n",
        "$$\n",
        "$$\n",
        "Av_{r+1}=0,\\ldots, Av_n=0.\n",
        "$$\n",
        "\n",
        "Hacer dibujo!\n",
        "\n",
        "---\n",
        "\n",
        "En forma matricial, si denotamos $V=(v_1,\\ldots,v_n)$ y $U=(u_1,\\ldots,u_m)$ tenemos\n",
        "$$\n",
        "AV = U \\Sigma\n",
        "\\quad\n",
        "A\n",
        "\\begin{bmatrix}\n",
        "v_1 & \\dots & v_r & \\dots & v_n\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "u_1 & \\dots & u_r & \\dots & u_m\n",
        "\\end{bmatrix}\n",
        "\\left[\n",
        "\\begin{array}{ccc|c}\n",
        "\\sigma_1 &        &        & 0 \\\\\n",
        "         & \\ddots &        & \\vdots \\\\\n",
        "         &        & \\sigma_r & 0 \\\\\n",
        "\\hline\n",
        "0        & \\cdots & 0        & 0\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Por lo que concluimos que\n",
        "$$\n",
        "A=U\\Sigma V^T,\n",
        "$$\n",
        "con $\\Sigma$ \"diagonal\", es decir $\\Sigma$ matriz $m\\times n$, de la forma $(D,0)$ o $\\binom{0}{D}$ con $D$ diagonal, dependiendo si $m$ es mayor o igual a $n$ o no.\n",
        "\n",
        "Es fácil ver que podemos descomponer en una suma de matrices de rango 1:\n",
        "$$\n",
        "A=\\sigma_1u_1v_1^T+\\cdots+ \\sigma_ru_rv_r^T\n",
        "$$"
      ],
      "metadata": {
        "id": "S1-4KPdzBiJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentarios:**\n",
        "- $u_1,\\ldots,u_r$ genera el espacio columnas de $A$.\n",
        "- $v_1,\\ldots,v_r$ genera el espacio filas de $A$.\n",
        "- $v_{r+1},\\ldots,v_m$ es una base del núcleo de $A$\n",
        "- $u_{r+1},\\ldots,u_m$ es una base del núcleo de $A^T$\n",
        "\n"
      ],
      "metadata": {
        "id": "P4jpnCQ7JejS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Lo más importante de la SVD:**\n",
        "La descomposición en valores singulares condensa la información geométrica de la matriz $A$. Además, tomando bases especiales del dominio y el codominio , se puede pensar que la matriz $A$ es diagonal.\n",
        "\n",
        "La matriz $A_1=\\sigma_1u_1v_1^T$ es la matriz de rango $1$ que mejor aproxima a $A$. Más en general tenemos:\n",
        "\n",
        "**Teorema de Eckart-Young:** La matriz\n",
        "$$\n",
        "A_k=\\sigma_1u_1v_1^T+\\cdots+ \\sigma_ku_kv_k^T\n",
        "$$\n",
        "es la matriz de rango $k$ que mejor aproxima a $A$.\n",
        "\n",
        "(siendo $\\|\\cdot\\|$ la norma de operador o de Frobenius.)\n",
        "\n",
        "\n",
        "\n",
        "Esto es, $A_k$ satisface\n",
        "$$\n",
        "\\|A-A_k\\|\\leq \\|A-B\\|,\\quad\\mbox{para toda matriz $B$ de rango $k$}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "Ox9TQcKnaUkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Forma reducida de SVD\n",
        "\n",
        "Una forma más concisa de expresar la descomposicón en valores singulares es excluir lo que me da cero. Esto es\n",
        "$$\n",
        "AV_r=U_r\\Sigma_r,\\quad A[v_1,\\ldots,v_r]=[u_1,\\ldots,u_r]\\cdot\\textrm{diag}(\\sigma_1,\\ldots,\\sigma_r).\n",
        "$$\n",
        "donde en este caso las matrices $V_r$ y $U_r$ son de Stiefel.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SHiIVkgUyF2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demostración de la descomposición SVD\n",
        "La prueba se deduce directamente del teorema espectral. Veamos esto:\n",
        "\n",
        "Sea $r$ el rango de $A$.\n",
        "\n",
        "Una primera observación es que la SVD de $A=U\\Sigma V^T$ implica que\n",
        "$$\n",
        "A^TA=(V\\Sigma^T U^T) U\\Sigma V^T =V\\Sigma^2 V^T \\\\\n",
        "AA^T = U\\Sigma V^T (V\\Sigma^T U^T) =U\\Sigma^2 U^T\n",
        "$$\n",
        "lo que sería la descomposición espectral de las matrices simétrica $A^TA$ y $AA^T$. Esto sugiere dos cosas\n",
        "- los vectores singulares a derecha (izquierda) son los vectores propios de $A^TA$ ($AA^T$).\n",
        "- los valores singulares son la raíz cuadrada de los valores propios de $AA^T$ o $A^TA$ (que además son los mismos).\n",
        "\n",
        "Con esto en mente hagamos la prueba, y logremos vincular los $u's$ con los $v's$.\n",
        "\n",
        "**Demostración:**\n",
        "\n",
        "1. La matriz $A^TA$ es simétrica (y semi difinida positiva) por lo que se diagonaliza en una base ortonormal $\\{v_1,\\ldots,v_n\\}$. Además los valores propios son no negativos, y por lo tanto podemos escribir\n",
        "$$\n",
        "A^TAv_i=\\sigma_i^2 v_i,\\qquad (i=1,\\ldots,n).\n",
        "$$\n",
        "(i.e., los valores singulares son las raíces cuadradas de los valores propios de $A^TA$.)\n",
        "Además como el rango de $A^TA$ coincide con el de $A$ (**por qué?**) podemos suponer $\\sigma_i>0$ para $i=1,\\ldots,r$ y $\\sigma_{r+1}=\\ldots=\\sigma_n=0$.\n",
        "\n",
        "2. Veamos que los vectores singulares a izquierda son las direcciones $Av_1,\\ldots,Av_n$. Observar que\n",
        "$$\n",
        "\\|Av_i\\|^2=v_i^TA^TAv_i=\\sigma_i^2v_i^Tv_i=\\sigma_i^2\n",
        "$$\n",
        "por lo que podemos definir los vectores unitarios\n",
        "$$\n",
        "u_i =\\frac{Av_i}{\\sigma_i},\\quad (i=1,\\ldots,r).\n",
        "$$\n",
        "(Observar que acá podemos definir hasta $u_1,\\ldots,u_r$,\n",
        "\n",
        " - **$u_i$ son ortogonales:** esto surge de que si $i\\neq j$\n",
        " $$\n",
        "v_i^TA^TAv_j={\\sigma_j}^2{v_i}^v_j =\\sigma_j^2\\delta_{i,j}=0\n",
        " $$\n",
        "3. Resta definir los vectores $u_{r+1},\\ldots,u_n$. Para esos vectores basta considerar una base ortonormal del epacio ortogonal al espacio columnas, i.e., el núcleo de $A^T$.\n",
        "\n",
        "\n",
        "\n",
        "Esto termina la prueba.\n",
        "\n",
        "Veamos que con esta construcción, los vectores singulares $u_i$ son vectores propios de $AA^T$:\n",
        "Esto surge de que\n",
        "$$\n",
        "AA^Tu_i=AA^T (\\frac{Av_i}{\\sigma_i})=A(A^TA )\\frac{v_i}{\\sigma_i}=\\sigma_iAv_i=\\sigma_i u_i.\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R82FKLgNbSqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e-WpOBVsEW4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comentarios:**\n",
        "- La prueba nos da una estrategia para encontrar la descomposición SVD. Se toma la matriz $A^TA$ y se diagonaliza. El procedimiento sigue como la prueba.\n",
        "- Un detalle no menor, pero que por el momento lo pasamos por alto, es que encontrar valores (y vectores) propios es un tema no tan sencillo. Pero ya hablaremos de esto.\n",
        "\n",
        "- **Pregunta**: Si $S = Q\\Lambda Q^T$ es simétrica definida positiva, ¿cuál es su SVD?  \n",
        "  <details>\n",
        "  <summary>Respuesta</summary>\n",
        "  La SVD es exactamente $U \\Sigma V^T = Q\\Lambda Q^T$. La matriz $U = V = Q$ es ortogonal.  \n",
        "  Y la matriz de valores propios $\\Lambda $ se convierte en la matriz de valores singulares $\\Sigma$.\n",
        "  </details>\n",
        "\n",
        "- **Pregunta**: Si $S = Q\\Lambda Q^T$ tiene un valor propio negativo ($Sx = -\\alpha x$), ¿cuál es el valor singular y cuáles son los vectores $v$ y $u$?  \n",
        "  <details>\n",
        "  <summary>Respuesta</summary>\n",
        "  El valor singular será $\\sigma = +\\alpha$ (positivo). Un vector singular (ya sea $u$ o $v$) debe ser $-x$ (cambiando el signo).  \n",
        "  Entonces $Sx = -\\alpha x$ es lo mismo que $Sv = \\sigma u$.  \n",
        "  Los dos cambios de signo se cancelan.\n",
        "  </details>\n",
        "\n",
        "- **Pregunta**: Si $A = Q$ es una matriz ortogonal, ¿por qué todos sus valores singulares son iguales a 1?  \n",
        "  <details>\n",
        "  <summary>Respuesta</summary>\n",
        "  Todos los valores singulares son $\\sigma = 1$ porque $A^T A = Q^T Q = I$. Entonces $\\Sigma = I$.  \n",
        "  Pero $U = Q$ y $V = I$ es solo una de las posibles elecciones para los vectores singulares $u$ y $v$:  \n",
        "  $$\n",
        "  Q = U \\Sigma V^T \\Rightarrow Q = Q I I^T \\text{ o cualquier } Q = (Q Q_1)I Q_1^T\n",
        "  $$\n",
        "  </details>\n",
        "\n",
        "- **Pregunta**: ¿Por qué todos los valores propios de una matriz cuadrada $A$ son menores o iguales que $\\sigma_1$?  \n",
        "  <details>\n",
        "  <summary>Respuesta</summary>\n",
        "  Multiplicar por matrices ortogonales $U$ y $V^T$ no cambia la norma de los vectores:\n",
        "  $$\n",
        "  \\|Ax\\| = \\|U \\Sigma V^T x\\| = \\|\\Sigma V^T x\\| \\leq \\sigma_1 \\|V^T x\\| = \\sigma_1 \\|x\\| \\quad \\text{para todo } x. \\tag{*}\n",
        "  $$\n",
        "  Un vector propio cumple $\\|Ax\\| = |\\lambda| \\|x\\|$. Entonces (*) da $|\\lambda| \\leq \\sigma_1$.\n",
        "  </details>\n",
        "\n",
        "- **Pregunta**: Si $A = xy^T$ tiene rango 1, ¿cuáles son $u_1$, $v_1$ y $\\sigma_1$? _Verificá que_ $|\\lambda_1| \\leq \\sigma_1$.  \n",
        "  <details>\n",
        "  <summary>Respuesta</summary>\n",
        "  Los vectores singulares son $u_1 = x / \\|x\\|$ y $v_1 = y / \\|y\\|$. Además calculando valores propios de $A^TA$ obtenemos $\\sigma_1 = \\|x\\| \\|y\\|$, que  es el único valor singular no nulo en la matriz $\\Sigma$.  \n",
        "  Aquí está la SVD:\n",
        "\n",
        "  #### Matriz de rango 1\n",
        "  $$\n",
        "  xy^T = \\frac{x}{\\|x\\|} (\\|x\\| \\|y\\|) \\frac{y^T}{\\|y\\|} = u_1 \\sigma_1 v_1^T\n",
        "  $$\n",
        "\n",
        "   </details>\n",
        "\n",
        " **Observación**: El único valor propio no nulo de $A = xy^T$ es $\\lambda = y^T x$ **Cuál es su vector propio?**.\n",
        "  Entonces $|\\lambda| = |y^T x| \\leq \\sigma_1 = \\|y\\| \\|x\\|$.\n",
        "\n",
        "  La desigualdad clave $|\\lambda_1| \\leq \\sigma_1$ es exactamente la desigualdad de Schwarz.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cMUZeW3PeSjO"
      }
    }
  ]
}