{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase22_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 22 (miércoles 17 de junio, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "eRyZv9o2Szwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducción Rápida de Dimensionalidad mediante Proyección Aleatoria | Lema de Johnson–Lindenstrauss\n",
        "\n",
        "\n",
        "## Introducción\n",
        "\n",
        "En problemas de alta dimensión, como el procesamiento de datos, aprendizaje automático o compresión, a menudo necesitamos reducir la dimensión de los datos sin perder demasiada información. Una forma sorprendentemente efectiva de hacerlo es mediante **proyecciones aleatorias**.\n",
        "\n",
        "Este método se basa en una idea central: **en alta dimensión, los vectores aleatorios son casi ortogonales**. Esto permite que proyecciones simples, definidas con matrices aleatorias, puedan preservar aproximadamente las distancias entre puntos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Imaginemos que necesitamos una reducción de dimensionalidad muy rápida, mediante una transformación o proyección desde un espacio de dimensión $m$ a un espacio de menor dimensión $k$.\n",
        "\n",
        "- ¡No disponemos de todos los datos ahora; no podemos calcular una matriz de covarianzas ni de disimilitudes!\n",
        "- ¡No podemos permitirnos calcular una transformación ortogonal, eso es demasiado costoso computacionalmente!\n",
        "\n",
        "¿Podemos simplemente transformar las muestras de entrenamiento a un espacio de menor dimensión usando una **transformación aleatoria**?\n",
        "\n"
      ],
      "metadata": {
        "id": "TPPQV9Pu5hJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🔁 Proyecciones y Matrices de Proyección\n",
        "\n",
        "Dada una matriz $J \\in \\mathbb{R}^{k \\times m}$ (con $k < m$), podemos definir una proyección de vectores de $\\mathbb{R}^m$ a $\\mathbb{R}^k$ como $x \\mapsto Jx$.\n",
        "\n",
        "Una matriz de proyección asociada se define como:\n",
        "\n",
        "$$\n",
        "P = J^\\top J \\in \\mathbb{R}^{m \\times m}\n",
        "$$\n",
        "\n",
        "Cuando las filas de $J$ son ortonormales, se tiene que:\n",
        "\n",
        "- $J J^\\top = I_k$\n",
        "- $P = J^\\top J$ es **simétrica** ($P = P^\\top$)\n",
        "- $P$ es **idempotente** ($P^2 = P$)\n",
        "\n",
        "Esto significa que $P$ proyecta ortogonalmente sobre el subespacio imagen de $J^\\top$.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎲 Proyecciones Aleatorias\n",
        "\n",
        "Supongamos ahora que cada entrada de $J$ se elige al azar, por ejemplo:\n",
        "\n",
        "$$\n",
        "J_{ij} \\sim \\mathcal{N}(0, 1/k)\n",
        "$$\n",
        "\n",
        "Es decir, cada componente se toma de una distribución normal con media cero y varianza $1/k$. En este caso, se puede demostrar que:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[J^\\top J] = I_m\n",
        "$$\n",
        "\n",
        "Esto implica que:\n",
        "\n",
        "- En promedio, las columnas de $J$ son ortogonales\n",
        "- Las longitudes se preservan en promedio:\n",
        "  \n",
        "$$\n",
        "\\mathbb{E}\\|Jx\\|^2 = \\|x\\|^2\n",
        "$$\n",
        "\n",
        "\n",
        "(Observar que $J$ tiene generalmente rango $k$, y por lo que $J^\\top J$ también tiene rango $k$, pero su esperanza tiene rango $m$. Da para pensar...)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 📐 Preservación de Distancias\n",
        "\n",
        "Una motivación clave es preservar distancias entre puntos $x_i$ y $x_j$ después de proyectar. Queremos que:\n",
        "\n",
        "$$\n",
        "\\|Jx_i - Jx_j\\|^2 \\approx \\|x_i - x_j\\|^2\n",
        "$$\n",
        "\n",
        "De lo anterior sabemos que para un par de puntos, eso se puede hacer (al menos en promedio). La dificultad ocurre que quiero preservar distancias pero entre muchos puntos. Observar que ya en $\\mathbb R^2$, tres puntos no alineados no pueden preservar su distancia mediante una proyección ortogonal en un subespacio de dimensión $1$.\n",
        "\n",
        "---\n",
        "\n",
        "## 📏 El Lema de Johnson–Lindenstrauss\n",
        "\n",
        "El siguiente es un resultado famoso que da una respuesta positiva a nuestro problema si agregamos cierta tolerancia.\n",
        "\n",
        "> Dado un conjunto de $n$ puntos $x_1, \\dots, x_n$ en $\\mathbb{R}^m$ y un error $\\epsilon \\in (0,1)$, si:\n",
        ">\n",
        "> $$\n",
        "> k \\geq \\frac{8 \\log n}{\\epsilon^2}\n",
        "> $$\n",
        ">\n",
        "> entonces existe una proyección lineal $J : \\mathbb{R}^m \\to \\mathbb{R}^k$ tal que para todos los pares $i,j$:\n",
        ">\n",
        "> $$\n",
        "> (1 - \\epsilon) \\|x_i - x_j\\|^2 \\leq \\|Jx_i - Jx_j\\|^2 \\leq (1 + \\epsilon) \\|x_i - x_j\\|^2\n",
        "> $$\n",
        "\n",
        "Esto implica que **se pueden reducir las dimensiones drásticamente manteniendo la estructura geométrica** del conjunto de puntos.\n",
        "\n",
        "Comentarios:\n",
        "- Observar que la dimensión $k$ requerida sólo depende de la cantidad de puntos y no del tamaño del espacio inicial.\n",
        "- La proyección se puede construir independiente de los datos.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Ejemplo en Julia\n",
        "\n",
        "Veamos un ejemplo práctico para ilustrar el lema.\n",
        "\n"
      ],
      "metadata": {
        "id": "buLE6v3k3lsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Random, LinearAlgebra, Statistics\n",
        "\n",
        "# Número de puntos y dimensión original\n",
        "n, m = 100, 1000\n",
        "X = randn(m, n)  # Cada columna es un punto en R^m\n",
        "\n",
        "# Parámetro de error deseado\n",
        "ϵ = 0.2\n",
        "k = ceil(Int, 8 * log(n) / ϵ^2)\n",
        "\n",
        "# Matriz de proyección aleatoria\n",
        "J = randn(k, m) / sqrt(k)\n",
        "Y = J * X  # Puntos proyectados en R^k\n",
        "\n",
        "# Función para medir la distorsión relativa promedio\n",
        "function distortion(X, Y)\n",
        "    n = size(X, 2)\n",
        "    d_orig = Float64[]\n",
        "    d_proj = Float64[]\n",
        "    for i in 1:n-1, j in i+1:n\n",
        "        push!(d_orig, norm(X[:, i] - X[:, j])^2)\n",
        "        push!(d_proj, norm(Y[:, i] - Y[:, j])^2)\n",
        "    end\n",
        "    rel_error = abs.(d_proj .- d_orig) ./ d_orig\n",
        "    return mean(rel_error)\n",
        "end\n",
        "\n",
        "println(\"Proyectando a dimensión k = $k\")\n",
        "println(\"Distorsión relativa promedio: \", distortion(X, Y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30mWeCep4Af0",
        "outputId": "b78aef4a-fd34-43cf-ce85-67b9c0d79ddd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proyectando a dimensión k = 922\n",
            "Distorsión relativa promedio: 0.03544154082506118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Más detalles\n",
        "\n",
        "- Se utiliza cuando los datos tienen una dimensión tan alta que es demasiado costoso realizar el cálculo.\n",
        "\n",
        "- Cuando la dimensionalidad es alta y el número de muestras es demasiado bajo como para calcular covarianzas de forma confiable.\n",
        "\n",
        "- Cuando no se tiene acceso al conjunto de datos completo (por ejemplo, en datos en tiempo real), la proyección se establece **sin ver los datos**:\n",
        "  - alternativa no adaptativa  \n",
        "  - no es necesario calcular todas las disimilitudes por pares por adelantado\n",
        "  - es muy barato y rápido computacionalmente.\n"
      ],
      "metadata": {
        "id": "a4WzMchq4sHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank one perturbation: Sherman-Morrison-Woodbury formula"
      ],
      "metadata": {
        "id": "jeL4s1Ae9o50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cómo varía la inversa de una matriz cuando a esta se le perturba? Comencemos con el caso más sencillo de hacer una perturabación, por matrices de rango $1$ de la identidad:\n",
        "\n",
        "Qué podemos decir de\n",
        "$$\n",
        "(\\textrm{Id}-uv^T)^{-1}\n",
        "$$\n",
        "\n",
        "Sea $B=\\textrm{Id}-uv^T$. Observar que:\n",
        "- $B$ restringido a $v^\\perp$ es la identidad;\n",
        "-$Bu= u-v^Tu u=(1-v^Tu)u$\n",
        "\n",
        "Luego tenemos que si $B$ es invertible (que sucede si $v^Tu\\neq 1$) (*porqué?),\n",
        "- $B^{-1}$ restringido a $v^\\perp$ es la identidad\n",
        "- $B^{-1}u=\\frac{1}{(1-v^Tu)}u$\n",
        "Esto sugiere que la inversa $B^{-1}$ es de la forma $\\textrm{Id}-\\alpha uv^T$.\n",
        "\n",
        "Un cálculo sencillo muestra que\n",
        "$$\n",
        "(\\textrm{Id}-uv^T)^{-1} = \\textrm{Id}+\\frac{1}{(1-v^Tu)} uv^T.\n",
        "$$\n",
        "\n",
        "Una forma similar ocurre si en vez de considerar una perturbaciń por matrices de rango uno, tomamos una perturbación tipo $UV^T$.  Todo esto es un caso particular de la siguiente fórmula:\n"
      ],
      "metadata": {
        "id": "kljda2LHGn1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula de Sherman-Morrison-Woodbury: **\n",
        "$$\n",
        "(A-UV^T)^{-1}=A^{-1} + A^{-1}U(\\textrm{Id}-V^TA^{-1}U)^{-1}V^TA^{-1}.\n",
        "$$\n",
        "\n",
        "Veamos una aplicación de esto."
      ],
      "metadata": {
        "id": "q_y5JZKXKBN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actualización de Mínimos Cuadrados\n",
        "\n",
        "\n",
        "Considere la ecuación normal de resolve el problema de mínimos cuadrados $\\|Ax-b\\|^2$:\n",
        "\n",
        "$$\n",
        "A^T A \\hat{x} = A^T b\n",
        "$$\n",
        "\n",
        "Supongamos que llega una nueva fila $r$ (de tamaño $1 \\times n$) y un nuevo dato $b_{m+1}$. Entonces, el sistema extendido se escribe como:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "A^T \\\\\n",
        "r^T\n",
        "\\end{bmatrix}^T\n",
        "\\begin{bmatrix}\n",
        "A & r^T\n",
        "\\end{bmatrix}\n",
        "\\hat{x}_{\\text{new}} =\n",
        "\\begin{bmatrix}\n",
        "A^T \\\\\n",
        "r^T\n",
        "\\end{bmatrix}^T\n",
        "\\begin{bmatrix}\n",
        "b \\\\\n",
        "b_{m+1}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "o simplemente:\n",
        "\n",
        "$$\n",
        "(A^T A + r^T r) \\hat{x}_{\\text{new}} = A^T b + r^T b_{m+1}\n",
        "$$\n",
        "\n",
        "La nueva matriz de normales es $A^T A + r^T r$, que es una **perturbación de rango 1** de $A^T A$.\n",
        "\n",
        "Para evitar recalcular todo, se usa la siguiente fórmula de actualización (Sherman–Morrison):\n",
        "\n",
        "$$\n",
        "(A^T A + r^T r)^{-1} =\n",
        "(A^T A)^{-1} - \\frac{(A^T A)^{-1} r^T r (A^T A)^{-1}}{1 + r (A^T A)^{-1} r^T}\n",
        "$$\n",
        "\n",
        "Sea $c = \\frac{1}{1 + r (A^T A)^{-1} r^T}$. Entonces:\n",
        "\n",
        "$$\n",
        "(A^T A + r^T r)^{-1}\n",
        "= (A^T A)^{-1} - c (A^T A)^{-1} r^T r (A^T A)^{-1}\n",
        "$$\n",
        "\n",
        "Para encontrar $c$, solo se necesita resolver $y = (A^T A)^{-1} r^T$.\n",
        "\n",
        "> Esta técnica permite actualizar la solución $\\hat{x}_{\\text{new}}$ sin recalcular todo el sistema. Si llegan $M$ nuevas filas en lugar de una, se puede aplicar de forma recursiva: **Mínimos Cuadrados Recursivos**.\n",
        "> Este tipo de análisis está relacionado al filtro de Kalman, que tiene como versión particular a lo recién visto, y es utilizado por ejemplo para el GPS."
      ],
      "metadata": {
        "id": "TB3OAcMIMEuN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iTABjxgTOCyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}