{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase22_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 22 (mi茅rcoles 17 de junio, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "eRyZv9o2Szwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducci贸n R谩pida de Dimensionalidad mediante Proyecci贸n Aleatoria | Lema de JohnsonLindenstrauss\n",
        "\n",
        "\n",
        "## Introducci贸n\n",
        "\n",
        "En problemas de alta dimensi贸n, como el procesamiento de datos, aprendizaje autom谩tico o compresi贸n, a menudo necesitamos reducir la dimensi贸n de los datos sin perder demasiada informaci贸n. Una forma sorprendentemente efectiva de hacerlo es mediante **proyecciones aleatorias**.\n",
        "\n",
        "Este m茅todo se basa en una idea central: **en alta dimensi贸n, los vectores aleatorios son casi ortogonales**. Esto permite que proyecciones simples, definidas con matrices aleatorias, puedan preservar aproximadamente las distancias entre puntos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Imaginemos que necesitamos una reducci贸n de dimensionalidad muy r谩pida, mediante una transformaci贸n o proyecci贸n desde un espacio de dimensi贸n $m$ a un espacio de menor dimensi贸n $k$.\n",
        "\n",
        "- 隆No disponemos de todos los datos ahora; no podemos calcular una matriz de covarianzas ni de disimilitudes!\n",
        "- 隆No podemos permitirnos calcular una transformaci贸n ortogonal, eso es demasiado costoso computacionalmente!\n",
        "\n",
        "驴Podemos simplemente transformar las muestras de entrenamiento a un espacio de menor dimensi贸n usando una **transformaci贸n aleatoria**?\n",
        "\n"
      ],
      "metadata": {
        "id": "TPPQV9Pu5hJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##  Proyecciones y Matrices de Proyecci贸n\n",
        "\n",
        "Dada una matriz $J \\in \\mathbb{R}^{k \\times m}$ (con $k < m$), podemos definir una proyecci贸n de vectores de $\\mathbb{R}^m$ a $\\mathbb{R}^k$ como $x \\mapsto Jx$.\n",
        "\n",
        "Una matriz de proyecci贸n asociada se define como:\n",
        "\n",
        "$$\n",
        "P = J^\\top J \\in \\mathbb{R}^{m \\times m}\n",
        "$$\n",
        "\n",
        "Cuando las filas de $J$ son ortonormales, se tiene que:\n",
        "\n",
        "- $J J^\\top = I_k$\n",
        "- $P = J^\\top J$ es **sim茅trica** ($P = P^\\top$)\n",
        "- $P$ es **idempotente** ($P^2 = P$)\n",
        "\n",
        "Esto significa que $P$ proyecta ortogonalmente sobre el subespacio imagen de $J^\\top$.\n",
        "\n",
        "---\n",
        "\n",
        "##  Proyecciones Aleatorias\n",
        "\n",
        "Supongamos ahora que cada entrada de $J$ se elige al azar, por ejemplo:\n",
        "\n",
        "$$\n",
        "J_{ij} \\sim \\mathcal{N}(0, 1/k)\n",
        "$$\n",
        "\n",
        "Es decir, cada componente se toma de una distribuci贸n normal con media cero y varianza $1/k$. En este caso, se puede demostrar que:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[J^\\top J] = I_m\n",
        "$$\n",
        "\n",
        "Esto implica que:\n",
        "\n",
        "- En promedio, las columnas de $J$ son ortogonales\n",
        "- Las longitudes se preservan en promedio:\n",
        "  \n",
        "$$\n",
        "\\mathbb{E}\\|Jx\\|^2 = \\|x\\|^2\n",
        "$$\n",
        "\n",
        "\n",
        "(Observar que $J$ tiene generalmente rango $k$, y por lo que $J^\\top J$ tambi茅n tiene rango $k$, pero su esperanza tiene rango $m$. Da para pensar...)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##  Preservaci贸n de Distancias\n",
        "\n",
        "Una motivaci贸n clave es preservar distancias entre puntos $x_i$ y $x_j$ despu茅s de proyectar. Queremos que:\n",
        "\n",
        "$$\n",
        "\\|Jx_i - Jx_j\\|^2 \\approx \\|x_i - x_j\\|^2\n",
        "$$\n",
        "\n",
        "De lo anterior sabemos que para un par de puntos, eso se puede hacer (al menos en promedio). La dificultad ocurre que quiero preservar distancias pero entre muchos puntos. Observar que ya en $\\mathbb R^2$, tres puntos no alineados no pueden preservar su distancia mediante una proyecci贸n ortogonal en un subespacio de dimensi贸n $1$.\n",
        "\n",
        "---\n",
        "\n",
        "##  El Lema de JohnsonLindenstrauss\n",
        "\n",
        "El siguiente es un resultado famoso que da una respuesta positiva a nuestro problema si agregamos cierta tolerancia.\n",
        "\n",
        "> Dado un conjunto de $n$ puntos $x_1, \\dots, x_n$ en $\\mathbb{R}^m$ y un error $\\epsilon \\in (0,1)$, si:\n",
        ">\n",
        "> $$\n",
        "> k \\geq \\frac{8 \\log n}{\\epsilon^2}\n",
        "> $$\n",
        ">\n",
        "> entonces existe una proyecci贸n lineal $J : \\mathbb{R}^m \\to \\mathbb{R}^k$ tal que para todos los pares $i,j$:\n",
        ">\n",
        "> $$\n",
        "> (1 - \\epsilon) \\|x_i - x_j\\|^2 \\leq \\|Jx_i - Jx_j\\|^2 \\leq (1 + \\epsilon) \\|x_i - x_j\\|^2\n",
        "> $$\n",
        "\n",
        "Esto implica que **se pueden reducir las dimensiones dr谩sticamente manteniendo la estructura geom茅trica** del conjunto de puntos.\n",
        "\n",
        "Comentarios:\n",
        "- Observar que la dimensi贸n $k$ requerida s贸lo depende de la cantidad de puntos y no del tama帽o del espacio inicial.\n",
        "- La proyecci贸n se puede construir independiente de los datos.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## И Ejemplo en Julia\n",
        "\n",
        "Veamos un ejemplo pr谩ctico para ilustrar el lema.\n",
        "\n"
      ],
      "metadata": {
        "id": "buLE6v3k3lsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Random, LinearAlgebra, Statistics\n",
        "\n",
        "# N煤mero de puntos y dimensi贸n original\n",
        "n, m = 100, 1000\n",
        "X = randn(m, n)  # Cada columna es un punto en R^m\n",
        "\n",
        "# Par谩metro de error deseado\n",
        "系 = 0.2\n",
        "k = ceil(Int, 8 * log(n) / 系^2)\n",
        "\n",
        "# Matriz de proyecci贸n aleatoria\n",
        "J = randn(k, m) / sqrt(k)\n",
        "Y = J * X  # Puntos proyectados en R^k\n",
        "\n",
        "# Funci贸n para medir la distorsi贸n relativa promedio\n",
        "function distortion(X, Y)\n",
        "    n = size(X, 2)\n",
        "    d_orig = Float64[]\n",
        "    d_proj = Float64[]\n",
        "    for i in 1:n-1, j in i+1:n\n",
        "        push!(d_orig, norm(X[:, i] - X[:, j])^2)\n",
        "        push!(d_proj, norm(Y[:, i] - Y[:, j])^2)\n",
        "    end\n",
        "    rel_error = abs.(d_proj .- d_orig) ./ d_orig\n",
        "    return mean(rel_error)\n",
        "end\n",
        "\n",
        "println(\"Proyectando a dimensi贸n k = $k\")\n",
        "println(\"Distorsi贸n relativa promedio: \", distortion(X, Y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30mWeCep4Af0",
        "outputId": "b78aef4a-fd34-43cf-ce85-67b9c0d79ddd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proyectando a dimensi贸n k = 922\n",
            "Distorsi贸n relativa promedio: 0.03544154082506118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### M谩s detalles\n",
        "\n",
        "- Se utiliza cuando los datos tienen una dimensi贸n tan alta que es demasiado costoso realizar el c谩lculo.\n",
        "\n",
        "- Cuando la dimensionalidad es alta y el n煤mero de muestras es demasiado bajo como para calcular covarianzas de forma confiable.\n",
        "\n",
        "- Cuando no se tiene acceso al conjunto de datos completo (por ejemplo, en datos en tiempo real), la proyecci贸n se establece **sin ver los datos**:\n",
        "  - alternativa no adaptativa  \n",
        "  - no es necesario calcular todas las disimilitudes por pares por adelantado\n",
        "  - es muy barato y r谩pido computacionalmente.\n"
      ],
      "metadata": {
        "id": "a4WzMchq4sHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank one perturbation: Sherman-Morrison-Woodbury formula"
      ],
      "metadata": {
        "id": "jeL4s1Ae9o50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C贸mo var铆a la inversa de una matriz cuando a esta se le perturba? Comencemos con el caso m谩s sencillo de hacer una perturabaci贸n, por matrices de rango $1$ de la identidad:\n",
        "\n",
        "Qu茅 podemos decir de\n",
        "$$\n",
        "(\\textrm{Id}-uv^T)^{-1}\n",
        "$$\n",
        "\n",
        "Sea $B=\\textrm{Id}-uv^T$. Observar que:\n",
        "- $B$ restringido a $v^\\perp$ es la identidad;\n",
        "-$Bu= u-v^Tu u=(1-v^Tu)u$\n",
        "\n",
        "Luego tenemos que si $B$ es invertible (que sucede si $v^Tu\\neq 1$) (*porqu茅?),\n",
        "- $B^{-1}$ restringido a $v^\\perp$ es la identidad\n",
        "- $B^{-1}u=\\frac{1}{(1-v^Tu)}u$\n",
        "Esto sugiere que la inversa $B^{-1}$ es de la forma $\\textrm{Id}-\\alpha uv^T$.\n",
        "\n",
        "Un c谩lculo sencillo muestra que\n",
        "$$\n",
        "(\\textrm{Id}-uv^T)^{-1} = \\textrm{Id}+\\frac{1}{(1-v^Tu)} uv^T.\n",
        "$$\n",
        "\n",
        "Una forma similar ocurre si en vez de considerar una perturbaci por matrices de rango uno, tomamos una perturbaci贸n tipo $UV^T$.  Todo esto es un caso particular de la siguiente f贸rmula:\n"
      ],
      "metadata": {
        "id": "kljda2LHGn1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula de Sherman-Morrison-Woodbury: **\n",
        "$$\n",
        "(A-UV^T)^{-1}=A^{-1} + A^{-1}U(\\textrm{Id}-V^TA^{-1}U)^{-1}V^TA^{-1}.\n",
        "$$\n",
        "\n",
        "Veamos una aplicaci贸n de esto."
      ],
      "metadata": {
        "id": "q_y5JZKXKBN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actualizaci贸n de M铆nimos Cuadrados\n",
        "\n",
        "\n",
        "Considere la ecuaci贸n normal de resolve el problema de m铆nimos cuadrados $\\|Ax-b\\|^2$:\n",
        "\n",
        "$$\n",
        "A^T A \\hat{x} = A^T b\n",
        "$$\n",
        "\n",
        "Supongamos que llega una nueva fila $r$ (de tama帽o $1 \\times n$) y un nuevo dato $b_{m+1}$. Entonces, el sistema extendido se escribe como:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "A^T \\\\\n",
        "r^T\n",
        "\\end{bmatrix}^T\n",
        "\\begin{bmatrix}\n",
        "A & r^T\n",
        "\\end{bmatrix}\n",
        "\\hat{x}_{\\text{new}} =\n",
        "\\begin{bmatrix}\n",
        "A^T \\\\\n",
        "r^T\n",
        "\\end{bmatrix}^T\n",
        "\\begin{bmatrix}\n",
        "b \\\\\n",
        "b_{m+1}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "o simplemente:\n",
        "\n",
        "$$\n",
        "(A^T A + r^T r) \\hat{x}_{\\text{new}} = A^T b + r^T b_{m+1}\n",
        "$$\n",
        "\n",
        "La nueva matriz de normales es $A^T A + r^T r$, que es una **perturbaci贸n de rango 1** de $A^T A$.\n",
        "\n",
        "Para evitar recalcular todo, se usa la siguiente f贸rmula de actualizaci贸n (ShermanMorrison):\n",
        "\n",
        "$$\n",
        "(A^T A + r^T r)^{-1} =\n",
        "(A^T A)^{-1} - \\frac{(A^T A)^{-1} r^T r (A^T A)^{-1}}{1 + r (A^T A)^{-1} r^T}\n",
        "$$\n",
        "\n",
        "Sea $c = \\frac{1}{1 + r (A^T A)^{-1} r^T}$. Entonces:\n",
        "\n",
        "$$\n",
        "(A^T A + r^T r)^{-1}\n",
        "= (A^T A)^{-1} - c (A^T A)^{-1} r^T r (A^T A)^{-1}\n",
        "$$\n",
        "\n",
        "Para encontrar $c$, solo se necesita resolver $y = (A^T A)^{-1} r^T$.\n",
        "\n",
        "> Esta t茅cnica permite actualizar la soluci贸n $\\hat{x}_{\\text{new}}$ sin recalcular todo el sistema. Si llegan $M$ nuevas filas en lugar de una, se puede aplicar de forma recursiva: **M铆nimos Cuadrados Recursivos**.\n",
        "> Este tipo de an谩lisis est谩 relacionado al filtro de Kalman, que tiene como versi贸n particular a lo reci茅n visto, y es utilizado por ejemplo para el GPS."
      ],
      "metadata": {
        "id": "TB3OAcMIMEuN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iTABjxgTOCyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}