{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase15_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 15 (Viernes 23 de mayo, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "X4LxHqF6tmVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factorizaci√≥n de Matrices\n"
      ],
      "metadata": {
        "id": "3lLeJQvKtslJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factorizaci√≥n de Matrices: Positividad y Esparsidad\n",
        "\n",
        "Tradicionalmente, se ha utilizado la descomposici√≥n en valores singulares (SVD) para descomponer una matriz $A$ como:\n",
        "$$\n",
        "A = U \\Sigma V^T\n",
        "$$\n",
        "Esto es muy √∫til, por ejemplo, en An√°lisis de Componentes Principales (PCA) como ya vimos en el curso.\n",
        "\n",
        "Sin embargo, en presencia de restricciones como **no negatividad** de las entradas de los factores o **esparsidad**, la SVD no siempre es adecuada.\n",
        "\n",
        "A continuaci√≥n, se presentan factorizaciones alternativas de $A$ que imponen nuevas propiedades √∫tiles:\n",
        "\n",
        "| Tipo de factorization     | Objetivo de minimizaci√≥n                              | Restricciones             |\n",
        "|---------------------------|--------------------------------------------------------|---------------------------|\n",
        "| **Matrices no negativas** | $$\\min \\|A - UV\\|_F^2$$                                 | $U \\geq 0$, $V \\geq 0$    |\n",
        "| **Esparsas y no negativas** | $$\\min \\|A - UV\\|_F^2 + \\lambda \\|UV\\|_N$$            | $U \\geq 0$, $V \\geq 0$    |\n",
        "\n",
        "### Algoritmo: Iteraci√≥n Alternante\n",
        "\n",
        "Para computar $A \\approx UV$, se utiliza una **iteraci√≥n alternante**:\n",
        "\n",
        "- **Actualizar $U$ con $V$ fijo**, luego\n",
        "- **Actualizar $V$ con $U$ fijo**.\n",
        "\n",
        "Cada paso es r√°pido ya que el problema es lineal en el factor que se est√° actualizando.\n",
        "\n",
        "Este enfoque es una extensi√≥n de la idea de SVD (donde $\\Sigma$ puede ser absorbido en $U$). La factorizaci√≥n $UV$ tambi√©n se relaciona con el **algoritmo de $k$-means**, agrupando vectores columna de $A$ en $r$ clusters.\n",
        "\n",
        "Esto es, si la columna $k$ de $A$ est√° en el cluster asociado al vector $u_j$, i.e., $a_k \\approx u_j$, entonces en la factorizaci√≥n $A\\approx UV$ se tiene que la columna $k$ de $V$,\n",
        "\n",
        "En resumen, estas factorizaciones permiten extraer estructuras interpretables y ajustadas a restricciones espec√≠ficas que la SVD no cumple.\n"
      ],
      "metadata": {
        "id": "hNapoqjkkQYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Proycto: Nonnegative Matrix Factorization (NMF)\n",
        "\n",
        "El objetivo de NMF es aproximar una matriz no negativa $A \\geq 0$ mediante un producto de rango bajo:\n",
        "$$\n",
        "A \\approx UV\n",
        "$$\n",
        "donde $U \\geq 0$ y $V \\geq 0$.\n",
        "\n",
        "### Prop√≥sito\n",
        "- **Rango bajo**: simplifica los datos.\n",
        "- **No negatividad**: asegura que las cantidades tengan sentido (volumen, probabilidad, conteo,distancia, etc.), evitando cancelaciones entre signos opuestos.\n",
        "\n",
        "### Dificultades\n",
        "Cuando $A$ es sim√©trica definida positiva, se podr√≠a esperar encontrar una matriz $B \\geq 0$ tal que $B^T B = A$, pero esto raramente ocurre (?). En su lugar, se puede buscar una matriz $B$ no negativa que minimice la diferencia con $A$ en el sentido de Frobenius:\n",
        "$$\n",
        "\\min_{B \\geq 0} \\|A - B^T B\\|_F\n",
        "$$\n",
        "\n",
        "### Aplicaciones\n",
        "- La factorizaci√≥n $A \\approx BC$ tambi√©n se utiliza en el contexto de la reducci√≥n de dimensionalidad, compresi√≥n y visualizaci√≥n de datos.\n",
        "- Si $C$ tiene menos columnas que $A$, se logra una representaci√≥n m√°s compacta.\n",
        "- Esta t√©cnica es √∫til cuando los datos son naturalmente no negativos y se desea interpretar sus componentes (por ejemplo, en im√°genes, documentos, biolog√≠a computacional).\n",
        "\n",
        "### Objetivos relacionados\n",
        "\n",
        "| M√©todo | Factorizaci√≥n buscada                          |\n",
        "|--------|------------------------------------------------|\n",
        "| NMF    | Encontrar matrices $U$, $V$ no negativas con $A \\approx UV$ |\n",
        "| SPCA   | Encontrar matrices esparsas $B$, $C$ con $A \\approx BC$     |\n",
        "\n",
        "### Referencias\n",
        "- Lee y Seung, *Nature* 401 (1999), 788‚Äì791.\n",
        "- N. Gillis, *The Why and How of Nonnegative Matrix Factorization*, arXiv:1401.5226.\n",
        "- L. Xu, B. Yu, Y. Zhang, *An alternating direction algorithm for matrix and tensor factorizations*, Computational Optimization and Applications, 68 (2017).\n"
      ],
      "metadata": {
        "id": "mO8nUaAanm48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Extracci√≥n de Rasgos Faciales\n",
        "\n",
        "En esta aplicaci√≥n, cada columna de la matriz $A$ representa una imagen facial, donde sus componentes corresponden a intensidades de p√≠xeles, por lo que $A \\geq 0$.\n",
        "\n",
        "- El objetivo es encontrar unas pocas **\"caras b√°sicas\"** contenidas en $B$, tal que:\n",
        "  $$\n",
        "  A \\approx BC\n",
        "  $$\n",
        "- Estas caras b√°sicas se combinan linealmente (sin cancelaciones negativas) para reconstruir las m√∫ltiples caras en $A$.\n",
        "- Variaciones en geometr√≠a (ojos, nariz, boca) pueden ser suficientes para una buena reconstrucci√≥n.\n",
        "- Esta idea est√° relacionada con el desarrollo de **eigenfaces** (Turk y Pentland), aunque aqu√≠ se enfatiza la **no negatividad** de los factores.\n",
        "\n",
        "Esta factorizaci√≥n permite representar rostros de forma interpretable, con partes b√°sicas que tienen sentido visual.\n"
      ],
      "metadata": {
        "id": "K4b90CUP44P5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Miner√≠a de Texto y Clasificaci√≥n de Documentos\n",
        "\n",
        "- En este contexto, cada **columna de $A$** representa un **documento**, y cada **fila** representa una **palabra**.\n",
        "- La matriz $A$ suele ser esparsa y no negativa (frecuencias o TF-IDF).\n",
        "- El objetivo es descomponer $A \\approx BC$, donde:\n",
        "\n",
        "  - $b_i$ es un **tema** (topic vector),\n",
        "  - $c_{ij}$ es la **importancia** del tema $b_i$ en el documento $a_j$.\n",
        "\n",
        "  $$\n",
        "  a_j \\approx \\sum_i c_{ij} \\cdot b_i\n",
        "  $$\n",
        "\n",
        "- Dado que $B \\geq 0$ y $C \\geq 0$, los documentos se construyen **combinando, no restando** temas.\n",
        "- Esto permite una interpretaci√≥n m√°s clara: NMF **identifica temas** y **clasifica** documentos en funci√≥n de ellos.\n",
        "\n",
        "## üß† Ejemplo de NMF para Miner√≠a de Texto\n",
        "\n",
        "Supongamos que tenemos 3 documentos y 4 palabras:\n",
        "\n",
        "- **Palabras**: `[\"data\", \"science\", \"football\", \"stadium\"]`\n",
        "- **Documentos**: `[\"Doc 1\", \"Doc 2\", \"Doc 3\"]`\n",
        "\n",
        "La matriz $A$ representa la frecuencia de cada palabra en cada documento:\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1 \\\\\\\\\n",
        "1 & 0 & 1 \\\\\\\\\n",
        "0 & 3 & 0 \\\\\\\\\n",
        "0 & 2 & 0 \\\\\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Esto significa:\n",
        "\n",
        "- Doc 1 y Doc 3 contienen palabras de data science.\n",
        "- Doc 2 contiene palabras de \"football\".\n",
        "\n",
        "### üéØ Objetivo\n",
        "\n",
        "Aplicamos **NMF** para factorizar $A \\approx BC$ con:\n",
        "\n",
        "$$ \\mbox{Documento }a_j\\approx \\sum_i (\\mbox{imporancia }c_{i,j}) (\\mbox{ tema } b_i)\n",
        "$$\n",
        "\n",
        "Luego $B$ puede pensarse como una colecci√≥n de temas, y estamos clasifcando todos los documentos como combinaci√≥n lineal de los temas m√°s importantes."
      ],
      "metadata": {
        "id": "p93aL_rxAASC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåø Sparse Principal Components (SPCA)\n",
        "\n",
        "En muchas aplicaciones (finanzas, gen√©tica, etc.) es importante tener **componentes principales dispersos**: es decir, vectores con **pocos elementos distintos de cero**. Esto permite:\n",
        "\n",
        "- Interpretabilidad (e.g. seleccionar unas pocas variables clave).\n",
        "- Evitar ruido o decisiones basadas en muchas variables con poco efecto.\n",
        "\n",
        "\n",
        "\n",
        "### Problema con SVD tradicional\n",
        "\n",
        "Los vectores singulares $u$ y $v$ de la SVD tienen **todos sus elementos distintos de cero**, lo que:\n",
        "\n",
        "- Dificulta la interpretaci√≥n.\n",
        "- Hace que no podamos seleccionar solo unas pocas variables significativas.\n",
        "\n",
        "\n",
        "\n",
        "### Formulaci√≥n b√°sica de SPCA\n",
        "\n",
        "Sea $A$ una matriz de datos, (o una matriz $S$ una matriz de covarianza muestral $S=\\frac{1}{n-1}X^TX$).\n",
        "\n",
        "Una idea ser√≠a resolver el siguiente problema de optimizaci√≥n\n",
        "$$\n",
        "\\max_{\\|x\\|=1} x^T S x - \\rho \\cdot \\text{Card}(x)\n",
        "$$\n",
        "\n",
        "O bien:\n",
        "\n",
        "$$\n",
        "\\max_{\\|x\\|=1} x^T S x \\quad \\text{sujeto a} \\quad \\text{Card}(x) \\leq k\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $S$ es la matriz de covarianza (sim√©trica y semi-definida positiva),\n",
        "- $\\text{Card}(x)$ es el n√∫mero de componentes no nulos de $x$.\n",
        "\n",
        "### Penalizaci√≥n con norma $\\ell^1$\n",
        "\n",
        "Como la **cardinalidad** no es eficiente para optimizaci√≥n, se reemplaza por una penalizaci√≥n $\\ell^1$, que favorece la dispersi√≥n:\n",
        "\n",
        "- Para vectores: usar $\\|x\\|_1$\n",
        "- Para matrices: usar la **norma nuclear** $\\|X\\|_N$ (suma de valores singulares)\n",
        "\n",
        "\n",
        "\n",
        "## LASSO: selecci√≥n de variables\n",
        "\n",
        "Para encontrar vectores dispersos $x$, se utiliza:\n",
        "\n",
        "$$\n",
        "\\min \\|Ax - b\\|^2 + \\lambda \\sum_{i=1}^n |x_i|\n",
        "$$\n",
        "\n",
        "Esto es **LASSO**, que favorece soluciones con muchos ceros en $x$.\n",
        "\n",
        "\n",
        "\n",
        "## Referencias para chusmear\n",
        "\n",
        "1. R. Tibshirani (1996), *Regression shrinkage and selection via the Lasso*\n",
        "2. H. Zou y T. Hastie (2005), *Regularization and variable selection via the elastic net*\n",
        "3. Zou, Hastie, Tibshirani (2006), *Sparse Principal Component Analysis*\n"
      ],
      "metadata": {
        "id": "1-rpdxL4EwZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JIjCwP7HEx0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}