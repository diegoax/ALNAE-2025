{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase18_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 18 (Mi√©rcoles 4 de junio, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "X4LxHqF6tmVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M√≠nimos cuadrados (Regresi√≥n Lineal/Polinomial)"
      ],
      "metadata": {
        "id": "QXe20Yl__wVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resolviendo $Ax = y$\n",
        "\n",
        "### Comentarios breves sobres sistemas lineales\n",
        "\n",
        "Sea $A\\in\\mathbb{R}^{m\\times n}$, $x\\in\\mathbb{R}^n$, $y\\in\\mathbb{R}^m$.\n",
        "\n",
        "\n",
        " Resultados clave sobre existencia y unicidad:\n",
        "\n",
        "1. *Existe* una soluci√≥n $x$ si y solo si $y \\in \\mathcal{C}(A)$.\n",
        "2. *Existe* una soluci√≥n $x$ para todo $y \\in \\mathbb{R}^m$ si y solo si $\\mathcal{C}(A) = \\mathbb{R}^m$.\n",
        "3. Una soluci√≥n $x$ es *√∫nica* si y solo si $\\mathcal{N}(A) = \\{0\\}$.\n",
        "4. Existe una √∫nica soluci√≥n para todo $y \\in \\mathbb{R}^m$ si y solo si $A$ es una matriz $m \\times m$ e inversible,  \n",
        "   *es decir*, ninguno de los valores propios o singulares de $A$ es cero.\n",
        "5. Hay a lo sumo una soluci√≥n para todo $y \\in \\mathbb{R}^m$ si y solo si las columnas de $A$ son linealmente independientes,  \n",
        "   *es decir*, $\\mathcal{N}(A) = \\{0\\}$, y esto solo es posible si $m \\ge n$.\n",
        "6. El sistema homog√©neo $Ax = 0$ tiene una soluci√≥n no trivial (*es decir*, distinta de cero) si y solo si $\\text{rango}(A) < n$.\n"
      ],
      "metadata": {
        "id": "brpCCDsZjyHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M√≠nimos Cuadrados y Pseudoinversa (Moore‚ÄìPenrose)\n",
        "\n",
        "## Motivaci√≥n: Interpolaci√≥n de Polinomios\n",
        "\n",
        "Comenzamos con una peque√±a digresi√≥n/motivaci√≥n sobre **interpolaci√≥n de polinomios**.\n",
        "\n",
        "### Interpolaci√≥n de $m$ puntos\n",
        "\n",
        "Supongamos que tenemos $m$ puntos $x_1, \\dots, x_m \\in \\mathbb{C}$  \n",
        "y datos $y_1, \\dots, y_m \\in \\mathbb{C}$.\n",
        "\n",
        "Existe un √∫nico polinomio de grado $m-1$:\n",
        "\n",
        "$$\n",
        "p(x) = c_0 + c_1 x + \\cdots + c_{m-1} x^{m-1}\n",
        "$$\n",
        "\n",
        "tal que interpola los datos:\n",
        "\n",
        "$$\n",
        "p(x_i) = y_i \\quad \\text{para } i = 1, \\dots, m.\n",
        "$$\n",
        "\n",
        "Esto equivale a resolver el sistema lineal:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1 & x_1 & x_1^2 & \\cdots & x_1^{m-1} \\\\\n",
        "1 & x_2 & x_2^2 & \\cdots & x_2^{m-1} \\\\\n",
        "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
        "1 & x_m & x_m^2 & \\cdots & x_m^{m-1}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_{m-1}\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Denotando por $V$ la matriz de Vandermonde:\n",
        "\n",
        "$$\n",
        "V =\n",
        "\\begin{pmatrix}\n",
        "1 & x_1 & \\cdots & x_1^{m-1} \\\\\n",
        "1 & x_2 & \\cdots & x_2^{m-1} \\\\\n",
        "\\vdots & \\vdots & & \\vdots \\\\\n",
        "1 & x_m & \\cdots & x_m^{m-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Entonces, los coeficientes $c_j$ del polinomio son soluci√≥n del sistema $Vc = y$.\n",
        "\n",
        "‚ö†Ô∏è **La matriz de Vandermonde es invertible si $x_i \\ne x_j$ para $i \\ne j$**.\n",
        "\n",
        "---\n",
        "\n",
        "### Ejercicio\n",
        "\n",
        "1. Probar que si $V$ es invertible y $Vc = 0$, entonces $p(x) = 0$.\n",
        "   - Es decir, si $p$ interpola $m$ ceros en $x_1, \\dots, x_m$ distintos, entonces $p \\equiv 0$.\n",
        "\n",
        "2. Probar que:\n",
        "\n",
        "$$\n",
        "\\det(V) = \\prod_{i<j}(x_j - x_i)\n",
        "$$\n",
        "\n",
        "**Idea:** considerar $f(x) = \\det(\\tilde{V})$ donde $\\tilde{V}$ es la matriz Vandermonde con una fila general $(1, x, x^2, \\dots)$ y aplicar desarrollo por cofactores.\n",
        "\n",
        "---\n",
        "\n",
        "Continuaremos viendo en la computadora qu√© sucede con los polinomios que interpolan.\n"
      ],
      "metadata": {
        "id": "Ks1fL9V9HFEe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3expR41Hu4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aproximaci√≥n por polinomios de grado menor\n",
        "\n",
        "De los ejemplos anteriores surge que el polinomio interpolador puede tener un comportamiento **no deseado** (e.g. oscilaciones no controladas, como en el fen√≥meno de Runge).\n",
        "\n",
        "### ¬øQu√© pasa si aproximamos por polinomios de grado menor?\n",
        "\n",
        "Tomemos $n < m$ y consideremos un polinomio de grado $n-1$:\n",
        "\n",
        "$$\n",
        "p(x) = c_0 + c_1 x + \\cdots + c_{n-1} x^{n-1}\n",
        "$$\n",
        "\n",
        "Buscamos $p$ tal que minimice el **error cuadr√°tico total** respecto a los datos $(x_i, y_i)$:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m |p(x_i) - y_i|^2 \\quad \\text{(lo m√°s chico posible)}\n",
        "$$\n",
        "\n",
        "Observaciones:\n",
        "\n",
        "- El sistema ya **no tiene soluci√≥n exacta** (es sobredeterminado).\n",
        "- Tenemos $n$ inc√≥gnitas (los coeficientes $c_0, \\dots, c_{n-1}$) y $m > n$ ecuaciones.\n",
        "- Lo que buscamos es **minimizar el error**, no anularlo.\n",
        "\n",
        "---\n",
        "\n",
        "## Casos particulares\n",
        "\n",
        "### Caso $n = 0$ (polinomio constante)\n",
        "\n",
        "Buscamos $c_0$ tal que:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m |c_0 - y_i|^2 \\quad \\text{sea m√≠nimo}\n",
        "$$\n",
        "\n",
        "‚úÖ La soluci√≥n es el **promedio**:\n",
        "\n",
        "$$\n",
        "c_0 = \\frac{1}{m} \\sum_{i=1}^m y_i\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Caso $n = 1$ (regresi√≥n lineal)\n",
        "\n",
        "Buscamos una recta $p(x) = c_0 + c_1 x$ que se ajuste a los datos en el sentido de m√≠nimos cuadrados:\n",
        "\n",
        "$$\n",
        "\\min_{c_0, c_1} \\sum_{i=1}^m |c_0 + c_1 x_i - y_i|^2\n",
        "$$\n",
        "\n",
        "Esto conduce al problema cl√°sico de **regresi√≥n lineal por m√≠nimos cuadrados**.\n"
      ],
      "metadata": {
        "id": "UKCtuc-HHvhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2 Regresi√≥n lineal y aprendizaje autom√°tico\n",
        "\n",
        "En la **regresi√≥n lineal**, se nos da un conjunto de datos de entrenamiento que consiste en $m$ vectores de entrada (caracter√≠sticas) $a_1, \\dots, a_m \\in \\mathbb{R}^n$  \n",
        "y respuestas correspondientes $y_1, \\dots, y_m \\in \\mathbb{R}$. Queremos encontrar coeficientes o pesos $x \\in \\mathbb{R}^n$ tales que $a_i^\\top x\n",
        "\\approx y_i$ <font color=\"blue\">se podr√≠a explicitar el t√©rmino independiente? igual no es grave, lo decimos en clase.</font>\n",
        ".  \n",
        "Apilando los vectores de entrada en una matriz $A$ y las respuestas en un vector $y$, se obtiene la siguiente expresi√≥n matricial:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}\n",
        "\\approx\n",
        "\\begin{bmatrix}\n",
        "a_1^\\top \\\\\n",
        "\\vdots \\\\\n",
        "a_m^\\top\n",
        "\\end{bmatrix}\n",
        "x,\n",
        "\\quad \\text{es decir,} \\quad\n",
        "y = Ax + \\varepsilon,\n",
        "$$\n",
        "\n",
        "donde $\\varepsilon \\in \\mathbb{R}^m$ representa el ruido u otros errores.\n",
        "\n",
        "Queremos determinar (o ‚Äúaprender‚Äù) el vector de pesos o coeficientes $x$ de forma que, dado un nuevo vector de caracter√≠sticas $a$, podamos **predecir** la respuesta correspondiente (desconocida) simplemente calculando $a^\\top x$.\n",
        "\n",
        "Este es un problema cl√°sico en estad√≠stica, y un m√©todo clave del \"aprendizaje autom√°tico\" que usualmente se debe considerar (para estimaci√≥n/regresi√≥n) antes de m√©todos m√°s complejos.\n",
        "\n",
        "La notaci√≥n para regresi√≥n lineal var√≠a seg√∫n la disciplina:\n",
        "\n",
        "- En estad√≠stica: $y \\approx X \\beta$\n",
        "- En aprendizaje autom√°tico: $y \\approx X w$\n",
        "- En √°lgebra lineal: $y \\approx Ax$\n",
        "\n",
        "---\n",
        "\n",
        "### Los nombres de las variables pueden variar:\n",
        "\n",
        "- **$y$**: respuesta, etiqueta, variable end√≥gena, variable medida, variable dependiente, variable predicha, ...\n",
        "- **$a$** (filas de $A$): caracter√≠sticas, regresores, variables ex√≥genas, explicativas, covariables, variables independientes, ...\n",
        "- **$x$**: vector de par√°metros, coeficientes de regresi√≥n, inc√≥gnitas, ...\n",
        "- **$\\varepsilon$**: ruido, error, perturbaci√≥n, ...\n",
        "\n",
        "> **Ejemplo:** Predecir la altura de un ni√±o a partir de la altura de sus padres. [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)\n"
      ],
      "metadata": {
        "id": "wbnkq6-JlAcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver [Demo](https://jefffessler.github.io/book-la-demo/generated/demos/05/ls-lift/) de Rao"
      ],
      "metadata": {
        "id": "kvJ8xyyUmpWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimaci√≥n por m√≠nimos cuadrados lineales\n",
        "\n",
        "En una situaci√≥n t√≠pica donde $m > n$ (es decir, el sistema est√° *sobredeterminado* o es \"alto\"), se puede mostrar que $\\mathcal{R}(A) \\ne \\mathbb{R}^m$.  \n",
        "Por lo tanto, habr√° (infinitos) vectores $y$ que no est√°n en el rango de $A$ y para los cuales **no existe soluci√≥n exacta** de $Ax = y$.\n",
        "\n",
        "Entonces, cuando $m > n$, en lugar de insistir en resolver exactamente $Ax = y$, buscamos una **soluci√≥n aproximada** tal que $Ax \\approx y$.  \n",
        "Para esto, necesitamos cuantificar qu√© tan \"cerca\" est√° una soluci√≥n.\n",
        "\n",
        "La soluci√≥n aproximada m√°s importante y com√∫n es usar la **estimaci√≥n por m√≠nimos cuadrados lineales (LLS)**, que consiste en encontrar un estimador $\\hat{x}$ que **mejor ajuste** los datos $y$ seg√∫n una norma eucl√≠dea:\n",
        "\n",
        "$$\n",
        "\\hat{x} = \\arg \\min_{x \\in \\mathbb{R}^n} \\|Ax - y\\|_2^2 \\tag{5.3}\n",
        "$$\n",
        "\n",
        "- $\\hat{x}$ se llama la **soluci√≥n por m√≠nimos cuadrados lineales**.\n",
        "- $Ax - y$ se llama el **residuo**.\n",
        "- $\\|Ax - y\\|_2$ es la **norma eucl√≠dea** del residuo, que mide el \"error\" del ajuste.\n",
        "- $\\arg\\min_x$ significa que buscamos el argumento $x$ que minimiza el error cuadr√°tico.\n",
        "- A menudo decimos ‚Äúminimizamos el error cuadr√°tico‚Äù cuando en realidad queremos decir que encontramos $\\hat{x}$ que lo minimiza.\n",
        "\n",
        "---\n",
        "\n",
        "### Ejemplo: el promedio\n",
        "\n",
        "Supongamos que observamos una medici√≥n ruidosa\n",
        "$$\n",
        "y_i=\\alpha +\\varepsilon_i,\\quad i = 1, \\dots, m.\n",
        "$$\n",
        "Por ejemplo, tomamos $m$ mediciones de una barra de metal.\n",
        "\n",
        "Estamos interesados en la soluci√≥n $\\alpha$ que minimiza\n",
        "$$\n",
        "\\|A\\alpha-y\\|^2,\n",
        "$$\n",
        "siendo $A=(1,\\ldots,1)^T$.\n",
        "Geom√©tricamente, nos podemos convencer que la soluci√≥n √≥ptima es la que hace que el vector $(1,\\ldots,1)^T \\alpha -y$ es ortogonal a $(1,\\ldots,1)^T$.\n",
        "\n",
        "Por lo tanto\n",
        "$$\n",
        "\\langle \\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\alpha-y,  \\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\rangle=0,\n",
        "$$\n",
        "lo que implica\n",
        "$$\n",
        "0=\\sum_{i=1}^m (\\alpha-y_i),\n",
        "$$\n",
        "y por lo tanto\n",
        "$$\n",
        "\\alpha =\\frac 1n \\sum_{i=1}^m y_i\n",
        "$$\n",
        "es el promedio de los datos.\n",
        "\n",
        "\n",
        "### Ejemplo: regresi√≥n polinomial\n",
        "Demos un paso m√°s.\n",
        "\n",
        "Supongamos que observamos muestras ruidosas de una se√±al:\n",
        "\n",
        "$$\n",
        "y_i = s(t_i) + \\varepsilon_i, \\quad i = 1, \\dots, m\n",
        "$$\n",
        "\n",
        "y creemos que la se√±al es un polinomio c√∫bico:\n",
        "\n",
        "$$\n",
        "s(t) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\alpha_3 t^3\n",
        "$$\n",
        "\n",
        "Entonces, en forma matricial:\n",
        "\n",
        "$$\n",
        "y \\approx Ax, \\quad \\text{donde} \\quad\n",
        "y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_M \\end{bmatrix}, \\quad\n",
        "A = \\begin{bmatrix}\n",
        "1 & t_1 & t_1^2 & t_1^3 \\\\\n",
        "1 & t_2 & t_2^2 & t_2^3 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "1 & t_m & t_m^2 & t_m^3\n",
        "\\end{bmatrix}, \\quad\n",
        "x = \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Este m√©todo se conoce como **regresi√≥n polinomial**, y las ‚Äúcaracter√≠sticas‚Äù son las potencias de los tiempos $t_m$.\n",
        "\n",
        "---\n",
        "\n",
        "### Notas adicionales\n",
        "\n",
        "- Si elegimos exactamente $m = 4$ valores distintos $t_i$, se puede resolver $Ax = y$ exactamente (si $A$ es invertible).\n",
        "- Pero si hay ruido, es preferible usar **$m \\gg 4$** muestras y estimar $\\hat{x}$ usando m√≠nimos cuadrados.\n",
        "- Para polinomios de mayor orden, a veces es mejor usar **polinomios ortogonales** como base en lugar de monomios.\n"
      ],
      "metadata": {
        "id": "hx99YYbToBwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distintos enfoques para resolver M√≠nimos Cuadrados\n",
        "\n",
        "Los m√©todos pueden depender de la complejidad de nuestro problema.\n",
        "\n",
        "1. Via SVD con la pseudoinversa de $A$, denotada por $A^\\dagger$.\n",
        "2. Ecuaciones normales: $A^TA\\hat x=A^Ty$\n",
        "3. Gram-Schimdt (o Householderr) $A=QR$\n",
        "4. Optimizaci√≥n/Gradientes.\n",
        "\n",
        "Veamos a continuaci√≥n un resumen de estos m√©todos."
      ],
      "metadata": {
        "id": "vBVG9TULoCdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. La pseudoinversa $A^+$ de una matriz $A$ (o inversa Moore-Penrose)\n",
        "\n",
        "### Idea general\n",
        "\n",
        "- Si $A$ es $m\\times n$, entonces $A^\\dagger$ es $n\\times m$, y se piensa como un mapa que \"deshace\" lo que hace $A$. (Va en el sentido inverso.)\n",
        "- Una forma de entender la pseudoinversa es la siguiente:\n",
        "Recordar que si restrigimos $A$ al ortogonal a su n√∫cleo (i.e. el espacio filas), entonces se puede ver como un isomorfismo entre ese espacio, y el espacio columnas. Luego $A^\\dagger$ es la aplicaci√≥n lineal que en el codominio proyecta ortogonalmente sobre la imagen de $A$, y luego invierte $A$ sobre el orgotonal al n√∫cleo (el isomorfismo que mencionamos).\n",
        "- Si $A$ es invertible, entonces $A^+ = A^{-1}$.\n",
        "- Para matrices no invertibles, $A^+$ act√∫a como un ‚Äúpseudo-inverso‚Äù.\n",
        "- $A^+$ invierte $A$ **cuando es posible**:\n",
        "  - Si $x$ est√° en el espacio fila ($x=A^Ty$): $A^+Ax = x$\n",
        "  - Si $b$ est√° en el espacio columna: $AA^+b = b$\n",
        "\n",
        "### Espacios asociados\n",
        "\n",
        "- **N√∫cleo de $A^+$** es el n√∫cleo de $A^T$ (i.e., el ortogonal a la imagen de $A$).\n",
        "- Vectores en el n√∫cleo son ortogonales a la imagen de $A$.\n",
        "- En estos casos, la mejor soluci√≥n en norma 2 a $Ax = y$ es $x^+ = A^+y$.\n",
        "\n",
        "### Ejemplo\n",
        "\n",
        "Si $A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0 \\end{bmatrix}$, entonces:\n",
        "\n",
        "$$\n",
        "A^+ = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Reglas para construir $A^+$\n",
        "\n",
        "- **Regla 1:** Si $A$ tiene columnas independientes, $A^+ = (A^TA)^{-1}A^T$\n",
        "- **Regla 2:** Si $A$ tiene filas independientes, $A^+ = A^T(AA^T)^{-1}$\n",
        "- **Regla 3:** Para $\\Sigma$ diagonal (SVD), se invierte donde hay valores singulares distintos de cero:\n",
        "\n",
        "  Si $\\Sigma = \\begin{bmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,  \n",
        "  entonces $\\Sigma^+ = \\begin{bmatrix} 1/\\sigma_1 & 0 & 0 \\\\ 0 & 1/\\sigma_2 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n",
        "\n",
        "### SVD y pseudoinversa\n",
        "\n",
        "Si $A = U \\Sigma V^T$, entonces:\n",
        "\n",
        "$$\n",
        "A^+ = V \\Sigma^+ U^T\n",
        "$$\n",
        "\n",
        "### Interpretaci√≥n geom√©trica\n",
        "\n",
        "- $A$ lleva el espacio fila al espacio columna.\n",
        "- $A^+$ revierte: lleva el espacio columna al espacio fila.\n",
        "- Los 4 subespacios fundamentales (imagen, n√∫cleo y sus ortogonales) se intercambian bajo $A$ y $A^+$.\n",
        "\n",
        "### Identidad √∫til\n",
        "\n",
        "$$\n",
        "A^+A = \\begin{bmatrix} I & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Donde $I$ act√∫a sobre el espacio fila de $A$ y el $0$ sobre el n√∫cleo.\n",
        "\n"
      ],
      "metadata": {
        "id": "RzZw-vHLEPEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¬øCu√°ndo es $A^T A$ invertible?\n",
        "\n",
        "La matriz $A^T A$ es invertible **exactamente cuando $A$ tiene columnas linealmente independientes**.\n",
        "\n",
        "- Si $Ax = 0$ implica $x = 0$, entonces $A^T A$ es invertible.\n",
        "- $A$ y $A^T A$ comparten el mismo n√∫cleo:  \n",
        "  $$ A^T A x = 0 \\Rightarrow x^T A^T A x = \\|Ax\\|^2 = 0 \\Rightarrow Ax = 0 $$\n",
        "\n",
        "Por tanto, para cualquier matriz $A$:\n",
        "\n",
        "- $\\mathcal{N}(A^T A) = \\mathcal{N}(A)$  \n",
        "- $\\mathcal{C}(AA^T) = \\mathcal{C}(A)$  \n",
        "- $\\mathrm{rank}(A^T A) = \\mathrm{rank}(AA^T) = \\mathrm{rank}(A)$\n",
        "\n",
        "---\n",
        "\n",
        "2. # Las ecuaciones normales\n",
        "\n",
        "El problema de m√≠nimos cuadrados consiste en resolver:\n",
        "\n",
        "$$ A^T A \\hat{x} = A^T b $$\n",
        "\n",
        "Cuando $Ax = b$ no tiene soluci√≥n, buscamos una $\\hat{x}$ que **minimice $\\|Ax - b\\|^2$**.\n",
        "\n",
        "## Geometr√≠a de la soluci√≥n\n",
        "\n",
        "- $p = A\\hat{x}$ es la **proyecci√≥n ortogonal** de $b$ sobre el espacio columna de $A$.\n",
        "- $e = b - p$ es el **error** ortogonal al plano.\n",
        "\n",
        "Esto implica:\n",
        "\n",
        "$$\n",
        "(Ax)^T (b - A\\hat{x}) = 0 \\quad \\forall x \\Rightarrow A^T (b - A\\hat{x}) = 0\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PrM-7fSbEP92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F√≥rmulas clave en m√≠nimos cuadrados\n",
        "\n",
        "#### 1. Ecuaci√≥n normal\n",
        "$$\n",
        "A^T A \\hat{x} = A^T b\n",
        "$$\n",
        "\n",
        "#### 2. Soluci√≥n de m√≠nimos cuadrados\n",
        "$$\n",
        "\\hat{x} = (A^T A)^{-1} A^T b\n",
        "$$\n",
        "\n",
        "#### 3. Matriz de proyecci√≥n ortogonal sobre espacio columnas de $A$\n",
        "$$\n",
        "A (A^T A)^{-1} A^T\n",
        "$$\n"
      ],
      "metadata": {
        "id": "GJkfmkoFMYYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Soluci via $A=QR$.\n",
        "\n",
        "Supongamos $A$ de rango m√°ximo para simplificar. Utilizando el m√©todo de Gram-Schmidt, (o via reflexiones de Householder), podemos escribir $A$ como\n",
        "$$\n",
        "A=QR,\n",
        "$$\n",
        "con $Q$ de Stiefel $R$ triangular superior (e invertible), y en este caso $R$ es invertible.\n",
        "\n",
        "Luego utilizando las ecuaciones normales tenemos que\n",
        "$$\n",
        "A^TA\\hat x=A^T b,\n",
        "$$\n",
        "por lo que\n",
        "$$\n",
        "(QR)^TQR \\hat x= R^TQ^Tb,\n",
        "$$\n",
        "es decir\n",
        "$$\n",
        "R^TQ^TQR \\hat x= R^TQ^Tb,\n",
        "$$\n",
        "por lo que obtenemos  $R^TR \\hat x= R^TQ^Tb,$\n",
        " y por lo tanto $$\n",
        "\\hat x= R^{-1}Q^Tb.\n",
        " $$\n",
        "\n"
      ],
      "metadata": {
        "id": "pObyiFD0N6FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Optimizaci√≥n\n",
        "\n"
      ],
      "metadata": {
        "id": "vd_kHC_1QOW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M√≠nimos Cuadrados como Problema de Optimizaci√≥n\n",
        "\n",
        "Queremos encontrar $\\hat{x}$ que minimice la funci√≥n de p√©rdida cuadr√°tica:\n",
        "\n",
        "$$\n",
        "f(x) = \\|Ax - b\\|^2 = (Ax - b)^T (Ax - b)\n",
        "$$\n",
        "\n",
        "### Derivada (gradiente) de $f(x)$\n",
        "\n",
        "Expandimos:\n",
        "\n",
        "$$\n",
        "f(x) = x^T A^T A x - 2 b^T A x + b^T b\n",
        "$$\n",
        "\n",
        "Tomamos el gradiente respecto a $x$:\n",
        "\n",
        "$$\n",
        "\\nabla f(x) = 2 A^T A x - 2 A^T b\n",
        "$$\n",
        "\n",
        "Igualando a cero:\n",
        "\n",
        "$$\n",
        "\\nabla f(x) = 0 \\quad \\Rightarrow \\quad A^T A x = A^T b\n",
        "$$\n",
        "\n",
        "üìå ¬°Esta es la ecuaci√≥n normal de m√≠nimos cuadrados!  \n",
        "La soluci√≥n $\\hat{x}$ es el **punto cr√≠tico** (m√≠nimo global, ya que $f$ es convexa).\n"
      ],
      "metadata": {
        "id": "XR9e393URFGd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhUSZ2c-RRMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}