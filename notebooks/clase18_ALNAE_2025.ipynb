{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase18_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 18 (Miércoles 4 de junio, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "X4LxHqF6tmVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mínimos cuadrados (Regresión Lineal/Polinomial)"
      ],
      "metadata": {
        "id": "QXe20Yl__wVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resolviendo $Ax = y$\n",
        "\n",
        "### Comentarios breves sobres sistemas lineales\n",
        "\n",
        "Sea $A\\in\\mathbb{R}^{m\\times n}$, $x\\in\\mathbb{R}^n$, $y\\in\\mathbb{R}^m$.\n",
        "\n",
        "\n",
        " Resultados clave sobre existencia y unicidad:\n",
        "\n",
        "1. *Existe* una solución $x$ si y solo si $y \\in \\mathcal{C}(A)$.\n",
        "2. *Existe* una solución $x$ para todo $y \\in \\mathbb{R}^m$ si y solo si $\\mathcal{C}(A) = \\mathbb{R}^m$.\n",
        "3. Una solución $x$ es *única* si y solo si $\\mathcal{N}(A) = \\{0\\}$.\n",
        "4. Existe una única solución para todo $y \\in \\mathbb{R}^m$ si y solo si $A$ es una matriz $m \\times m$ e inversible,  \n",
        "   *es decir*, ninguno de los valores propios o singulares de $A$ es cero.\n",
        "5. Hay a lo sumo una solución para todo $y \\in \\mathbb{R}^m$ si y solo si las columnas de $A$ son linealmente independientes,  \n",
        "   *es decir*, $\\mathcal{N}(A) = \\{0\\}$, y esto solo es posible si $m \\ge n$.\n",
        "6. El sistema homogéneo $Ax = 0$ tiene una solución no trivial (*es decir*, distinta de cero) si y solo si $\\text{rango}(A) < n$.\n"
      ],
      "metadata": {
        "id": "brpCCDsZjyHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mínimos Cuadrados y Pseudoinversa (Moore–Penrose)\n",
        "\n",
        "## Motivación: Interpolación de Polinomios\n",
        "\n",
        "Comenzamos con una pequeña digresión/motivación sobre **interpolación de polinomios**.\n",
        "\n",
        "### Interpolación de $m$ puntos\n",
        "\n",
        "Supongamos que tenemos $m$ puntos $x_1, \\dots, x_m \\in \\mathbb{C}$  \n",
        "y datos $y_1, \\dots, y_m \\in \\mathbb{C}$.\n",
        "\n",
        "Existe un único polinomio de grado $m-1$:\n",
        "\n",
        "$$\n",
        "p(x) = c_0 + c_1 x + \\cdots + c_{m-1} x^{m-1}\n",
        "$$\n",
        "\n",
        "tal que interpola los datos:\n",
        "\n",
        "$$\n",
        "p(x_i) = y_i \\quad \\text{para } i = 1, \\dots, m.\n",
        "$$\n",
        "\n",
        "Esto equivale a resolver el sistema lineal:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1 & x_1 & x_1^2 & \\cdots & x_1^{m-1} \\\\\n",
        "1 & x_2 & x_2^2 & \\cdots & x_2^{m-1} \\\\\n",
        "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
        "1 & x_m & x_m^2 & \\cdots & x_m^{m-1}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_{m-1}\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Denotando por $V$ la matriz de Vandermonde:\n",
        "\n",
        "$$\n",
        "V =\n",
        "\\begin{pmatrix}\n",
        "1 & x_1 & \\cdots & x_1^{m-1} \\\\\n",
        "1 & x_2 & \\cdots & x_2^{m-1} \\\\\n",
        "\\vdots & \\vdots & & \\vdots \\\\\n",
        "1 & x_m & \\cdots & x_m^{m-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Entonces, los coeficientes $c_j$ del polinomio son solución del sistema $Vc = y$.\n",
        "\n",
        "⚠️ **La matriz de Vandermonde es invertible si $x_i \\ne x_j$ para $i \\ne j$**.\n",
        "\n",
        "---\n",
        "\n",
        "### Ejercicio\n",
        "\n",
        "1. Probar que si $V$ es invertible y $Vc = 0$, entonces $p(x) = 0$.\n",
        "   - Es decir, si $p$ interpola $m$ ceros en $x_1, \\dots, x_m$ distintos, entonces $p \\equiv 0$.\n",
        "\n",
        "2. Probar que:\n",
        "\n",
        "$$\n",
        "\\det(V) = \\prod_{i<j}(x_j - x_i)\n",
        "$$\n",
        "\n",
        "**Idea:** considerar $f(x) = \\det(\\tilde{V})$ donde $\\tilde{V}$ es la matriz Vandermonde con una fila general $(1, x, x^2, \\dots)$ y aplicar desarrollo por cofactores.\n",
        "\n",
        "---\n",
        "\n",
        "Continuaremos viendo en la computadora qué sucede con los polinomios que interpolan.\n"
      ],
      "metadata": {
        "id": "Ks1fL9V9HFEe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3expR41Hu4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aproximación por polinomios de grado menor\n",
        "\n",
        "De los ejemplos anteriores surge que el polinomio interpolador puede tener un comportamiento **no deseado** (e.g. oscilaciones no controladas, como en el fenómeno de Runge).\n",
        "\n",
        "### ¿Qué pasa si aproximamos por polinomios de grado menor?\n",
        "\n",
        "Tomemos $n < m$ y consideremos un polinomio de grado $n-1$:\n",
        "\n",
        "$$\n",
        "p(x) = c_0 + c_1 x + \\cdots + c_{n-1} x^{n-1}\n",
        "$$\n",
        "\n",
        "Buscamos $p$ tal que minimice el **error cuadrático total** respecto a los datos $(x_i, y_i)$:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m |p(x_i) - y_i|^2 \\quad \\text{(lo más chico posible)}\n",
        "$$\n",
        "\n",
        "Observaciones:\n",
        "\n",
        "- El sistema ya **no tiene solución exacta** (es sobredeterminado).\n",
        "- Tenemos $n$ incógnitas (los coeficientes $c_0, \\dots, c_{n-1}$) y $m > n$ ecuaciones.\n",
        "- Lo que buscamos es **minimizar el error**, no anularlo.\n",
        "\n",
        "---\n",
        "\n",
        "## Casos particulares\n",
        "\n",
        "### Caso $n = 0$ (polinomio constante)\n",
        "\n",
        "Buscamos $c_0$ tal que:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m |c_0 - y_i|^2 \\quad \\text{sea mínimo}\n",
        "$$\n",
        "\n",
        "✅ La solución es el **promedio**:\n",
        "\n",
        "$$\n",
        "c_0 = \\frac{1}{m} \\sum_{i=1}^m y_i\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Caso $n = 1$ (regresión lineal)\n",
        "\n",
        "Buscamos una recta $p(x) = c_0 + c_1 x$ que se ajuste a los datos en el sentido de mínimos cuadrados:\n",
        "\n",
        "$$\n",
        "\\min_{c_0, c_1} \\sum_{i=1}^m |c_0 + c_1 x_i - y_i|^2\n",
        "$$\n",
        "\n",
        "Esto conduce al problema clásico de **regresión lineal por mínimos cuadrados**.\n"
      ],
      "metadata": {
        "id": "UKCtuc-HHvhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2 Regresión lineal y aprendizaje automático\n",
        "\n",
        "En la **regresión lineal**, se nos da un conjunto de datos de entrenamiento que consiste en $m$ vectores de entrada (características) $a_1, \\dots, a_m \\in \\mathbb{R}^n$  \n",
        "y respuestas correspondientes $y_1, \\dots, y_m \\in \\mathbb{R}$. Queremos encontrar coeficientes o pesos $x \\in \\mathbb{R}^n$ tales que $a_i^\\top x\n",
        "\\approx y_i$.  \n",
        "Apilando los vectores de entrada en una matriz $A$ y las respuestas en un vector $y$, se obtiene la siguiente expresión matricial:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}\n",
        "\\approx\n",
        "\\begin{bmatrix}\n",
        "a_1^\\top \\\\\n",
        "\\vdots \\\\\n",
        "a_m^\\top\n",
        "\\end{bmatrix}\n",
        "x,\n",
        "\\quad \\text{es decir,} \\quad\n",
        "y = Ax + \\varepsilon,\n",
        "$$\n",
        "\n",
        "donde $\\varepsilon \\in \\mathbb{R}^m$ representa el ruido u otros errores.\n",
        "\n",
        "Queremos determinar (o “aprender”) el vector de pesos o coeficientes $x$ de forma que, dado un nuevo vector de características $a$, podamos **predecir** la respuesta correspondiente (desconocida) simplemente calculando $a^\\top x$.\n",
        "\n",
        "Este es un problema clásico en estadística, y un método clave del \"aprendizaje automático\" que usualmente se debe considerar (para estimación/regresión) antes de métodos más complejos.\n",
        "\n",
        "La notación para regresión lineal varía según la disciplina:\n",
        "\n",
        "- En estadística: $y \\approx X \\beta$\n",
        "- En aprendizaje automático: $y \\approx X w$\n",
        "- En álgebra lineal: $y \\approx Ax$\n",
        "\n",
        "---\n",
        "\n",
        "### Los nombres de las variables pueden variar:\n",
        "\n",
        "- **$y$**: respuesta, etiqueta, variable endógena, variable medida, variable dependiente, variable predicha, ...\n",
        "- **$a$** (filas de $A$): características, regresores, variables exógenas, explicativas, covariables, variables independientes, ...\n",
        "- **$x$**: vector de parámetros, coeficientes de regresión, incógnitas, ...\n",
        "- **$\\varepsilon$**: ruido, error, perturbación, ...\n",
        "\n",
        "> **Ejemplo:** Predecir la altura de un niño a partir de la altura de sus padres. [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)\n"
      ],
      "metadata": {
        "id": "wbnkq6-JlAcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver [Demo](https://jefffessler.github.io/book-la-demo/generated/demos/05/ls-lift/) de Rao"
      ],
      "metadata": {
        "id": "kvJ8xyyUmpWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimación por mínimos cuadrados lineales\n",
        "\n",
        "En una situación típica donde $m > n$ (es decir, el sistema está *sobredeterminado* o es \"alto\"), se puede mostrar que $\\mathcal{R}(A) \\ne \\mathbb{R}^m$.  \n",
        "Por lo tanto, habrá (infinitos) vectores $y$ que no están en el rango de $A$ y para los cuales **no existe solución exacta** de $Ax = y$.\n",
        "\n",
        "Entonces, cuando $m > n$, en lugar de insistir en resolver exactamente $Ax = y$, buscamos una **solución aproximada** tal que $Ax \\approx y$.  \n",
        "Para esto, necesitamos cuantificar qué tan \"cerca\" está una solución.\n",
        "\n",
        "La solución aproximada más importante y común es usar la **estimación por mínimos cuadrados lineales (LLS)**, que consiste en encontrar un estimador $\\hat{x}$ que **mejor ajuste** los datos $y$ según una norma euclídea:\n",
        "\n",
        "$$\n",
        "\\hat{x} = \\arg \\min_{x \\in \\mathbb{R}^n} \\|Ax - y\\|_2^2 \\tag{5.3}\n",
        "$$\n",
        "\n",
        "- $\\hat{x}$ se llama la **solución por mínimos cuadrados lineales**.\n",
        "- $Ax - y$ se llama el **residuo**.\n",
        "- $\\|Ax - y\\|_2$ es la **norma euclídea** del residuo, que mide el \"error\" del ajuste.\n",
        "- $\\arg\\min_x$ significa que buscamos el argumento $x$ que minimiza el error cuadrático.\n",
        "- A menudo decimos “minimizamos el error cuadrático” cuando en realidad queremos decir que encontramos $\\hat{x}$ que lo minimiza.\n",
        "\n",
        "---\n",
        "\n",
        "### Ejemplo: el promedio\n",
        "\n",
        "Supongamos que observamos una medición ruidosa\n",
        "$$\n",
        "y_i=\\alpha +\\varepsilon_i,\\quad i = 1, \\dots, m.\n",
        "$$\n",
        "Por ejemplo, tomamos $m$ mediciones de una barra de metal.\n",
        "\n",
        "Estamos interesados en la solución $\\alpha$ que minimiza\n",
        "$$\n",
        "\\|A\\alpha-y\\|^2,\n",
        "$$\n",
        "siendo $A=(1,\\ldots,1)^T$.\n",
        "Geométricamente, nos podemos convencer que la solución óptima es la que hace que el vector $(1,\\ldots,1)^T \\alpha -y$ es ortogonal a $(1,\\ldots,1)^T$.\n",
        "\n",
        "Por lo tanto\n",
        "$$\n",
        "\\langle \\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\alpha-y,  \\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\rangle=0,\n",
        "$$\n",
        "lo que implica\n",
        "$$\n",
        "0=\\sum_{i=1}^m (\\alpha-y_i),\n",
        "$$\n",
        "y por lo tanto\n",
        "$$\n",
        "\\alpha =\\frac 1n \\sum_{i=1}^m y_i\n",
        "$$\n",
        "es el promedio de los datos.\n",
        "\n",
        "\n",
        "### Ejemplo: regresión polinomial\n",
        "Demos un paso más.\n",
        "\n",
        "Supongamos que observamos muestras ruidosas de una señal:\n",
        "\n",
        "$$\n",
        "y_i = s(t_i) + \\varepsilon_i, \\quad i = 1, \\dots, m\n",
        "$$\n",
        "\n",
        "y creemos que la señal es un polinomio cúbico:\n",
        "\n",
        "$$\n",
        "s(t) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\alpha_3 t^3\n",
        "$$\n",
        "\n",
        "Entonces, en forma matricial:\n",
        "\n",
        "$$\n",
        "y \\approx Ax, \\quad \\text{donde} \\quad\n",
        "y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_M \\end{bmatrix}, \\quad\n",
        "A = \\begin{bmatrix}\n",
        "1 & t_1 & t_1^2 & t_1^3 \\\\\n",
        "1 & t_2 & t_2^2 & t_2^3 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "1 & t_m & t_m^2 & t_m^3\n",
        "\\end{bmatrix}, \\quad\n",
        "x = \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Este método se conoce como **regresión polinomial**, y las “características” son las potencias de los tiempos $t_m$.\n",
        "\n",
        "---\n",
        "\n",
        "### Notas adicionales\n",
        "\n",
        "- Si elegimos exactamente $m = 4$ valores distintos $t_i$, se puede resolver $Ax = y$ exactamente (si $A$ es invertible).\n",
        "- Pero si hay ruido, es preferible usar **$m \\gg 4$** muestras y estimar $\\hat{x}$ usando mínimos cuadrados.\n",
        "- Para polinomios de mayor orden, a veces es mejor usar **polinomios ortogonales** como base en lugar de monomios.\n"
      ],
      "metadata": {
        "id": "hx99YYbToBwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distintos enfoques para resolver Mínimos Cuadrados\n",
        "\n",
        "Los métodos pueden depender de la complejidad de nuestro problema.\n",
        "\n",
        "1. Via SVD con la pseudoinversa de $A$, denotada por $A^\\dagger$.\n",
        "2. Ecuaciones normales: $A^TA\\hat x=A^Ty$\n",
        "3. Gram-Schimdt (o Householderr) $A=QR$\n",
        "4. Optimización/Gradientes.\n",
        "\n",
        "Veamos a continuación un resumen de estos métodos."
      ],
      "metadata": {
        "id": "vBVG9TULoCdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. La pseudoinversa $A^+$ de una matriz $A$ (o inversa Moore-Penrose)\n",
        "\n",
        "### Idea general\n",
        "\n",
        "- Si $A$ es $m\\times n$, entonces $A^\\dagger$ es $n\\times m$, y se piensa como un mapa que \"deshace\" lo que hace $A$. (Va en el sentido inverso.)\n",
        "- Una forma de entender la pseudoinversa es la siguiente:\n",
        "Recordar que si restrigimos $A$ al ortogonal a su núcleo (i.e. el espacio filas), entonces se puede ver como un isomorfismo entre ese espacio, y el espacio columnas. Luego $A^\\dagger$ es la aplicación lineal que en el codominio proyecta ortogonalmente sobre la imagen de $A$, y luego invierte $A$ sobre el orgotonal al núcleo (el isomorfismo que mencionamos).\n",
        "- Si $A$ es invertible, entonces $A^+ = A^{-1}$.\n",
        "- Para matrices no invertibles, $A^+$ actúa como un “pseudo-inverso”.\n",
        "- $A^+$ invierte $A$ **cuando es posible**:\n",
        "  - Si $x$ está en el espacio fila ($x=A^Ty$): $A^+Ax = x$\n",
        "  - Si $b$ está en el espacio columna: $AA^+b = b$\n",
        "\n",
        "### Espacios asociados\n",
        "\n",
        "- **Núcleo de $A^+$** es el núcleo de $A^T$ (i.e., el ortogonal a la imagen de $A$).\n",
        "- Vectores en el núcleo son ortogonales a la imagen de $A$.\n",
        "- En estos casos, la mejor solución en norma 2 a $Ax = y$ es $x^+ = A^+y$.\n",
        "\n",
        "### Ejemplo\n",
        "\n",
        "Si $A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0 \\end{bmatrix}$, entonces:\n",
        "\n",
        "$$\n",
        "A^+ = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Reglas para construir $A^+$\n",
        "\n",
        "- **Regla 1:** Si $A$ tiene columnas independientes, $A^+ = (A^TA)^{-1}A^T$\n",
        "- **Regla 2:** Si $A$ tiene filas independientes, $A^+ = A^T(AA^T)^{-1}$\n",
        "- **Regla 3:** Para $\\Sigma$ diagonal (SVD), se invierte donde hay valores singulares distintos de cero:\n",
        "\n",
        "  Si $\\Sigma = \\begin{bmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,  \n",
        "  entonces $\\Sigma^+ = \\begin{bmatrix} 1/\\sigma_1 & 0 & 0 \\\\ 0 & 1/\\sigma_2 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n",
        "\n",
        "### SVD y pseudoinversa\n",
        "\n",
        "Si $A = U \\Sigma V^T$, entonces:\n",
        "\n",
        "$$\n",
        "A^+ = V \\Sigma^+ U^T\n",
        "$$\n",
        "\n",
        "### Interpretación geométrica\n",
        "\n",
        "- $A$ lleva el espacio fila al espacio columna.\n",
        "- $A^+$ revierte: lleva el espacio columna al espacio fila.\n",
        "- Los 4 subespacios fundamentales (imagen, núcleo y sus ortogonales) se intercambian bajo $A$ y $A^+$.\n",
        "\n",
        "### Identidad útil\n",
        "\n",
        "$$\n",
        "A^+A = \\begin{bmatrix} I & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "es decir, es la proyección ortogonal sobre el espacio filas (i.e., el ortogonal al núcleo).\n",
        "\n",
        "Además\n",
        "$$\n",
        "AA^\\dagger=\\mbox{proyección ortogonal sobre espacio columna de }A.\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RzZw-vHLEPEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Cuándo es $A^T A$ invertible?\n",
        "\n",
        "La matriz $A^T A$ es invertible **exactamente cuando $A$ tiene columnas linealmente independientes**.\n",
        "\n",
        "- Si $Ax = 0$ implica $x = 0$, entonces $A^T A$ es invertible:- $A$ y $A^T A$ comparten el mismo núcleo. Esto sucdede porque\n",
        "  $$ A^T A x = 0 \\Rightarrow x^T A^T A x = \\|Ax\\|^2 = 0 \\Rightarrow Ax = 0 $$\n",
        "\n",
        "Por tanto, para cualquier matriz $A$:\n",
        "\n",
        "- $\\mathcal{N}(A^T A) = \\mathcal{N}(A)$  \n",
        "- $\\mathcal{C}(AA^T) = \\mathcal{C}(A)$  \n",
        "- $\\mathrm{rank}(A^T A) = \\mathrm{rank}(AA^T) = \\mathrm{rank}(A)$\n",
        "\n",
        "---\n",
        "\n",
        "2. # Las ecuaciones normales\n",
        "\n",
        "El problema de mínimos cuadrados consiste en resolver:\n",
        "\n",
        "$$ A^T A \\hat{x} = A^T b $$\n",
        "\n",
        "Cuando $Ax = b$ no tiene solución, buscamos una $\\hat{x}$ que **minimice $\\|Ax - b\\|^2$**.\n",
        "\n",
        "## Geometría de la solución\n",
        "\n",
        "- $p = A\\hat{x}$ es la **proyección ortogonal** de $b$ sobre el espacio columna de $A$.\n",
        "- $e = b - p$ es el **error** ortogonal al plano.\n",
        "\n",
        "Esto implica:\n",
        "\n",
        "$$\n",
        "(Ax)^T (b - A\\hat{x}) = 0 \\quad \\forall x \\Rightarrow A^T (b - A\\hat{x}) = 0\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PrM-7fSbEP92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fórmulas clave en mínimos cuadrados\n",
        "\n",
        "#### 1. Ecuación normal\n",
        "$$\n",
        "A^T A \\hat{x} = A^T b\n",
        "$$\n",
        "\n",
        "#### 2. Solución de mínimos cuadrados\n",
        "$$\n",
        "\\hat{x} = (A^T A)^{-1} A^T b\n",
        "$$\n",
        "\n",
        "#### 3. Matriz de proyección ortogonal sobre espacio columnas de $A$\n",
        "$$\n",
        "A (A^T A)^{-1} A^T\n",
        "$$\n"
      ],
      "metadata": {
        "id": "GJkfmkoFMYYr"
      }
    }
  ]
}