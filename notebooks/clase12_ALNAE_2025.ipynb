{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNw5SnMZBN/F1e83ANNS36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegoax/ALNAE-2025/blob/main/notebooks/clase12_ALNAE_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 12 (Miércoles 30 de abril, 2025)\n",
        "---"
      ],
      "metadata": {
        "id": "X4LxHqF6tmVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVD continuado"
      ],
      "metadata": {
        "id": "yK0ac9WxWLGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Geometría de la SVD\n",
        "\n",
        "Hemos probado que toda matriz $A$ se puede escribir como $A=U\\Sigma V^T$, con $U,V$ ortogonales y $\\Sigma$ diagonal. Consideremos para simplificar la versión reducida, $A=U_r\\Sigma_r V_r^T$.\n",
        "\n",
        "Observar que lo que geométricamente nos está diciendo es que existe una base **ortonormal** del dominio (la dada por las columnas de $V_r$), que va en una base **ortogonal** del codominio (la dada por las columnas de $\\Sigma_rU_r$, es decir las columnas $\\sigma_iu_i$).\n",
        "\n",
        "Veamos esto en coordenadas, y supongamos $A$ de rango máximo para facilitar notaciones:\n",
        "\n",
        "- Sea $x\\in\\mathbb{R}^n$\n",
        "- $V^Tx=\n",
        "\\begin{pmatrix}\n",
        "c_1\\\\\\vdots\\\\c_n\n",
        "\\end{pmatrix}\n",
        "$ que son los coeficinetes de $x$ en la base ortonormal dada por los $v_i$: $$x=c_1v_1+\\cdots +c_nv_n.$$\n",
        "- $\\Sigma V^Tx=\n",
        "\\begin{pmatrix}\\sigma_1 \\\\\\vdots\\\\\\sigma_nc_n\n",
        "\\end{pmatrix}$\n",
        "- $U\\Sigma V^Tx= \\sigma_1 c_1u_1+\\cdots+\\sigma_nc_nu_n.$\n",
        "\n",
        "En resumen, si damos un vector $x\\in\\mathbb{R}^n$ en la base de los $v_i's$ (i.e., los coeficientes $c's$), entonces el vector $Ax$ en la base de los $u_i's$ es un operador diagonal: cada coeficiente $c_i$ se manda a $\\sigma_ic_i$.\n",
        "\n",
        "$$\n",
        "x=c_1v_1+\\cdots +c_nv_n  \\quad \\overset{A}{\\longmapsto} \\quad Ax=  \\sigma_1 c_1u_1+\\cdots+\\sigma_nc_nu_n.\n",
        "$$\n",
        "\n",
        "\n",
        "En un dibujo, es más fácil decirlo :)\n"
      ],
      "metadata": {
        "id": "ciuNxu8thKO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El primer vector singular $v_1$\n",
        "\n",
        "El siguiente paso establece una nueva forma de ver a $v_1$. En lo anterior, elegimos los $v$ como vectores propios de $A^T A$.\n",
        "\n",
        "Pero si recuerdan, de la prueba del Teorema espectral que vimos en la\n",
        "[Colab de Clase 8](https://github.com/diegoax/ALNAE-2025/blob/main/notebooks/clase8_ALNAE_2025.ipynb)\n",
        " (utilizando $S=A^TA$ como matriz simétrica), $v_1$ es la solución de\n",
        "$$\n",
        "\\arg\\max_{\\|x\\|=1}\\|Ax\\|^2\n",
        "$$\n",
        "o lo que es lo mismo que\n",
        "> **Maximizar el cociente** $\\dfrac{\\|Ax\\|}{\\|x\\|}$, $x\\neq 0$.\n",
        "\n",
        "Conociendo la SVD, es claro que el **máximo es** $\\sigma_1$  y **se aclanza en el vector** $x=v_1$. Pero esto mismo lo sabíamos antes, dado que el máximo sería $\\|Av_1\\|^2=v_1A^TAv_1=\\lambda_1$ que es el el primer valor propio.\n",
        "\n",
        "Los otros vectores singulares se encuentran considerando la restricicón de $\\|Ax\\|/\\|x\\|$ a $v_1^\\perp$, luego a $v_1^\\perp \\cap v_2^\\perp$ y así sucesivamente, y maximando en cada caso.\n",
        "\n"
      ],
      "metadata": {
        "id": "loESm02MMREb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valores propios de matrices $AB$ y $BA$\n",
        "\n",
        "Hemos visto que los valores propios (no nulos) de $A^TA$ son los mismos que $AA^T$, y son los valores singluares de $A$ (o $A^T$).\n",
        "\n",
        "Veamos algo un poco más general.\n",
        "\n",
        "Supongamos que tengo dos matrices $A$ y $B$ tales que $AB$ y $BA$ tiene sentido.\n",
        "Entonces, los valores propios no nulos, son los mismos.\n",
        "\n",
        "<details><summary>Demostración:</summary>\n",
        "Esto surge de que si $ABx=\\lambda x$, con $\\lambda \\neq 0$, entonces\n",
        "$$\n",
        "BA(Bx)= B(AB)x=B(\\lambda x)=\\lambda Bx\n",
        "$$\n",
        "y por lo tanto $Bx$ es un vector propio de $AB$ con valor propio $\\lambda$. Es decir todo valor propio no nulo de $AB$ es valor propoio de $BA$. (Empezando al revés, i.e. con $BA$ tenemos la doble inclusión.)\n",
        "</details>"
      ],
      "metadata": {
        "id": "dKBRt0S_WPWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La SVD para derivadas e integrales\n",
        "\n",
        "Este puede ser el ejemplo más claro de la SVD. No comienza con una matriz (pero llegaremos ahí). Históricamente, la primera SVD no fue para vectores sino para **funciones**. Entonces $A$ no es una matriz sino un **operador**.  \n",
        "Un ejemplo es el operador que **integra** cada función. Otro ejemplo es el operador (no acotado) $D$ que toma la derivada:\n",
        "\n",
        "> ### **Operadores sobre funciones**  \n",
        "> **Integral y derivada**  \n",
        "> $Ax(s) = \\displaystyle \\int_0^s x(t)\\, dt$  \n",
        "> y  \n",
        "> $Dx(t) = \\dfrac{dx}{dt}$\n",
        "\n",
        "Esos operadores son lineales (o el cálculo sería mucho más difícil de lo que es). En cierto sentido, $D$ es el inverso de $A$, por el Teorema Fundamental del Cálculo. Más exactamente, $D$ es un inverso a izquierda con $DA = I$: la derivada de la integral es la función original.\n",
        "\n",
        "Pero $AD \\ne I$ porque la derivada de una función constante es cero. Entonces $D$ tiene un **núcleo nulo**, como una matriz con columnas linealmente dependientes.  \n",
        "**$D$ es la seudoinversa de $A$**. Los senos y cosenos son los $u$'s y $v$'s para $A$ = integral y $D$ = derivada:\n",
        "\n",
        "> $A v = \\sigma u$ es  \n",
        "> $$A(\\cos kt) = \\frac{1}{k} (\\sin kt)$$  \n",
        "> Entonces  \n",
        "> $$D(\\sin kt) = k (\\cos kt)$$\n",
        "\n",
        "La simplicidad de estas ecuaciones es la razón para incluir este ejemplo en el libro. Estamos trabajando con **funciones periódicas**: $x(t + 2\\pi) = x(t)$.  \n",
        "El espacio de entrada para $A$ contiene las **funciones pares** como $\\cos t = \\cos(-t)$.  \n",
        "Las salidas de $A$ (y entradas para $D$) son **funciones impares** como $\\sin t = -\\sin(-t)$.  \n",
        "Esos espacios de entrada y salida son como $\\mathbb{R}^n$ y $\\mathbb{R}^m$ para una matriz $m \\times n$.\n",
        "\n",
        "---\n",
        "\n",
        "La propiedad especial de la SVD es que los $v$'s son ortogonales, y los $u$'s también.  \n",
        "Aquí esos vectores singulares se han convertido en funciones muy agradables:\n",
        "\n",
        "> _Los cosenos son ortogonales entre sí, y también los senos. Sus productos internos son integrales que valen cero:_\n",
        "\n",
        "$$\n",
        "v_k^T v_j = \\int_0^{2\\pi} (\\cos kt)(\\cos jt) dt = 0, \\quad\n",
        "u_k^T u_j = \\int_0^{2\\pi} (\\sin kt)(\\sin jt) dt = 0.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Notar que el producto interno de funciones $\\mathbf{x}_1$ y $\\mathbf{x}_2$ es la integral de $x_1(t)\\, x_2(t)$.  \n",
        "> Esto se transfiere al espacio de funciones (**espacio de Hilbert**), el producto punto que suma $y \\cdot z = \\sum y_i z_i$.  \n",
        "> De hecho, el símbolo $\\int$ proviene de alguna manera de $\\sum$ (y las integrales son el límite de sumas).\n"
      ],
      "metadata": {
        "id": "qROguPGKTP7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>**Versión discreta de la derivada:  Diferencias Finitas**</summary>\n",
        "\n",
        "La forma discreta de una derivada es una **diferencia finita**.  \n",
        "La forma discreta de una integral es una **suma**.  \n",
        "Aquí elegimos una matriz $D$ de $4 \\times 3$ que corresponde a la diferencia hacia atrás  \n",
        "$f(x) - f(x - \\Delta x)$:\n",
        "\n",
        "$$\n",
        "D = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "-1 & 1 \\\\\n",
        "& -1 & 1 \\\\\n",
        "& & -1\n",
        "\\end{bmatrix}\n",
        "\\quad \\text{con} \\quad\n",
        "D^T = \\begin{bmatrix}\n",
        "1 & -1 & & \\\\\n",
        "1 & & -1 & \\\\\n",
        "& & 1 & -1\n",
        "\\end{bmatrix}\n",
        "\\tag{25}\n",
        "$$\n",
        "\n",
        "Para encontrar valores y vectores singulares, computamos $D^T D$ (de $3 \\times 3$) y $D D^T$ (de $4 \\times 4$):\n",
        "\n",
        "$$\n",
        "D^T D =\n",
        "\\begin{bmatrix}\n",
        "2 & -1 & 0 \\\\\n",
        "-1 & 2 & -1 \\\\\n",
        "0 & -1 & 2\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "D D^T =\n",
        "\\begin{bmatrix}\n",
        "1 & -1 & 0 & 0 \\\\\n",
        "-1 & 2 & -1 & 0 \\\\\n",
        "0 & -1 & 2 & -1 \\\\\n",
        "0 & 0 & -1 & 1\n",
        "\\end{bmatrix}\n",
        "\\tag{26}\n",
        "$$\n",
        "\n",
        "Los autovalores **no nulos** son siempre los mismos.  \n",
        "$D D^T$ también tiene un autovalor **cero** con autovector  \n",
        "$$\n",
        "u_4 = \\left(\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2} \\right).\n",
        "$$\n",
        "\n",
        "Esto es el equivalente discreto de la función $f(x) = \\frac{1}{2}$ con $df/dx = 0$.\n",
        "\n",
        "Los autovalores **no nulos** de ambas matrices simétricas $D^T D$ y $D D^T$ son:\n",
        "\n",
        "$$\n",
        "\\lambda_1 = \\sigma_1^2(D) = 2 + \\sqrt{2}, \\quad\n",
        "\\lambda_2 = \\sigma_2^2(D) = 2, \\quad\n",
        "\\lambda_3 = \\sigma_3^2(D) = 2 - \\sqrt{2}\n",
        "\\tag{27}\n",
        "$$\n",
        "\n",
        "Los autovectores $v$ de $D^T D$ son los **vectores singulares derechos** de $D$. Son **senos discretos**.  \n",
        "Los autovectores $u$ de $D D^T$ son los **vectores singulares izquierdos** de $D$. Son **cosenos discretos**.\n",
        "\n",
        "---\n",
        "\n",
        "$$\n",
        "\\sqrt{2} V =\n",
        "\\begin{bmatrix}\n",
        "\\sin \\frac{\\pi}{4} & \\sin \\frac{2\\pi}{4} & \\sin \\frac{3\\pi}{4} \\\\\n",
        "\\sin \\frac{2\\pi}{4} & \\sin \\frac{4\\pi}{4} & \\sin \\frac{6\\pi}{4} \\\\\n",
        "\\sin \\frac{3\\pi}{4} & \\sin \\frac{6\\pi}{4} & \\sin \\frac{9\\pi}{4}\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\sqrt{2} U =\n",
        "\\begin{bmatrix}\n",
        "\\cos \\frac{1\\pi}{2 \\cdot 4} & \\cos \\frac{2\\pi}{2 \\cdot 4} & \\cos \\frac{3\\pi}{2 \\cdot 4} & 1 \\\\\n",
        "\\cos \\frac{3\\pi}{2 \\cdot 4} & \\cos \\frac{6\\pi}{2 \\cdot 4} & \\cos \\frac{9\\pi}{2 \\cdot 4} & 1 \\\\\n",
        "\\cos \\frac{5\\pi}{2 \\cdot 4} & \\cos \\frac{10\\pi}{2 \\cdot 4} & \\cos \\frac{15\\pi}{2 \\cdot 4} & 1 \\\\\n",
        "\\cos \\frac{7\\pi}{2 \\cdot 4} & \\cos \\frac{14\\pi}{2 \\cdot 4} & \\cos \\frac{21\\pi}{2 \\cdot 4} & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Estas son las matrices famosas **DST** y **DCT**:  \n",
        "**Transformada Discreta del Seno** y **Transformada Discreta del Coseno**.\n",
        "\n",
        "La matriz **DCT** ha sido la base de la compresión de imágenes JPEG.  \n",
        "De hecho, JPEG incrementa $U$ a $8 \\times 8$, lo cual reduce la “bloqueosidad” de la imagen.  \n",
        "Los bloques de $8 \\times 8$ píxeles son transformados por una **DCT bidimensional**, luego comprimidos y transmitidos.\n",
        "\n",
        "La **ortogonalidad** de estas matrices es la clave en la Sección IV.4.\n",
        "\n",
        "---\n",
        "\n",
        "## I.8 Singular Values and Singular Vectors in the SVD — Página 67\n",
        "\n",
        "Nuestro objetivo era mostrar la forma discreta de la hermosa descomposición en valores singulares (SVD)  \n",
        "$D(\\sin kt) = k (\\cos kt)$.  \n",
        "Podés decir correctamente que este es solo un ejemplo.  \n",
        "Pero Fourier siempre está presente para ecuaciones lineales con coeficientes constantes — y siempre importante.\n",
        "\n",
        "En procesamiento de señales, las letras clave son **LTI**:  \n",
        "**Linealidad e Invariancia en el Tiempo (Linear Time Invariance)**.\n"
      ],
      "metadata": {
        "id": "w_G10gOFTUKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components and the Best Low Rank Matrix\n",
        "\n",
        "Los componentes principales de $A$ son sus vectores singulares, las columnas $u_j$ y $v_j$ de las matrices ortogonales $U$ y $V$. El **Análisis de Componentes Principales (PCA)** usa los $\\sigma$ más grandes conectados a los primeros $u$’s y $v$’s para entender la información en una matriz de datos.\n",
        "\n",
        "Dada una matriz $A$, extraemos su parte más importante $A_k$ (los $\\sigma$ más grandes):\n",
        "\n",
        "$$\n",
        "A_k = \\sigma_1 u_1 v_1^T + \\cdots + \\sigma_k u_k v_k^T \\quad \\text{con } \\operatorname{rank}(A_k) = k.\n",
        "$$\n",
        "\n",
        "$A_k$ resuelve un problema de optimización matricial. **La matriz de rango $k$ más cercana a $A$ es $A_k$**. En estadística, esto equivale a identificar las partes de $A$ con mayor varianza.\n",
        "\n",
        "En este contexto, el PCA es aprendizaje “no supervisado”. Nuestra única herramienta es el álgebra lineal: la SVD nos dice cómo elegir $A_k$. En aprendizaje supervisado, tenemos un gran conjunto de datos etiquetado.\n",
        "\n",
        "---\n",
        "\n",
        "### Propiedad clave del mejor rango-$k$:\n",
        "\n",
        "> **Teorema de Eckart-Young**  \n",
        "> Si $B$ tiene rango $k$, entonces  \n",
        "> $$\n",
        "> \\|A - B\\| \\geq \\|A - A_k\\|.\n",
        "> $$\n",
        "\n",
        "---\n",
        "\n",
        "### Normas matriciales especiales:\n",
        "\n",
        "Tres elecciones comunes para la norma matricial $\\|A\\|$:\n",
        "\n",
        "- **Norma espectral**:\n",
        "  $$\n",
        "  \\|A\\|_2 = \\max \\frac{\\|Ax\\|}{\\|x\\|} = \\sigma_1 \\quad \\text{(norma $ℓ^2$)}\n",
        "  \\tag{2}\n",
        "  $$\n",
        "\n",
        "- **Norma de Frobenius**:\n",
        "  $$\n",
        "  \\|A\\|_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}\n",
        "  \\tag{3}\n",
        "  $$\n",
        "\n",
        "- **Norma nuclear** (o traza):\n",
        "  $$\n",
        "  \\|A\\|_N = \\sigma_1 + \\sigma_2 + \\cdots + \\sigma_r\n",
        "  \\tag{4}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### Valores de normas para la matriz identidad $n \\times n$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\|I\\|_2 &= 1, & \\|I\\|_F &= \\sqrt{n}, & \\|I\\|_N &= n. \\tag{5}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Para cualquier matriz ortogonal $Q$, estas normas se mantienen:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\|QI\\|_2 &= 1, & \\|QI\\|_F &= \\sqrt{n}, & \\|QI\\|_N &= n. \\tag{6}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Además, las normas espectral, de Frobenius y nuclear permanecen invariantes cuando $A$ se multiplica (a izquierda o derecha) por una matriz ortogonal.\n"
      ],
      "metadata": {
        "id": "mRbUuL5Xkfd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Invarianza por transformaciones ortogonales\n",
        "\n",
        "Los valores singulares **no cambian** cuando $U$ y $V$ cambian a $Q_1 U$ y $Q_2 V$. Para matrices complejas, usamos la palabra *unitario* en lugar de *ortogonal*. Entonces $Q^T Q = I$.\n",
        "\n",
        "Estas tres normas son **unitariamente invariantes**:\n",
        "\n",
        "$$\n",
        "\\| Q_1 A Q_2^T \\| = \\|A\\| \\quad \\text{(para cualquier norma unitaria)}\n",
        "$$\n",
        "\n",
        "La demostración del teorema de Eckart-Young (ecuación (1)) se aplica también a todas las normas unitariamente invariantes: $\\|A\\|$ es computable a partir de $\\Sigma$.\n",
        "\n",
        "**Todas estas normas satisfacen**:\n",
        "\n",
        "$$\n",
        "\\|Q_1 A Q_2^T\\| = \\|A\\| \\quad \\text{para } Q_1, Q_2 \\text{ ortogonales.} \\tag{7}\n",
        "$$\n",
        "\n",
        "Ahora damos pruebas más simples de (1) para la norma $L^2$ y la norma de Frobenius.\n",
        "\n",
        "---\n",
        "\n",
        "### Teorema de Eckart-Young: Mejor Aproximación por $A_k$\n",
        "\n",
        "Veamos qué dice el teorema antes de demostrarlo. En este ejemplo, $A$ es diagonal y $k = 2$:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "4 & 0 & 0 & 0 \\\\\n",
        "0 & 3 & 0 & 0 \\\\\n",
        "0 & 0 & 2 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\quad \\text{la matriz de rango 2 más cercana a } A \\text{ es } \\quad\n",
        "A_2 =\n",
        "\\begin{bmatrix}\n",
        "4 & 0 & 0 & 0 \\\\\n",
        "0 & 3 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Esto debe ser cierto. Uno podría pensar que esta matriz diagonal es demasiado simple, pero la norma $L^2$ y la norma de Frobenius no cambian cuando la matriz $A$ se transforma en $Q_1 A Q_2$ (para matrices ortogonales $Q_1$ y $Q_2$).\n",
        "\n",
        "Este ejemplo incluye cualquier matriz $4 \\times 4$ con valores singulares $4, 3, 2, 1$. El teorema de Eckart-Young nos dice que debemos conservar $4$ y $3$ porque son los más grandes.\n",
        "\n",
        "- La **norma $L^2$** de $A - A_2$ es:\n",
        "  $$\n",
        "  \\|A - A_2\\| = 2.\n",
        "  $$\n",
        "\n",
        "- La **norma de Frobenius** de $A - A_2$ es:\n",
        "  $$\n",
        "  \\|A - A_2\\|_F = \\sqrt{5}.\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### ¿Podría otra matriz de rango 2 ser mejor?\n",
        "\n",
        "El problema es que el conjunto de matrices de rango 2 **no es convexo**. El promedio de $A_2$ y $B_2$ (ambas de rango 2) puede tener rango 4.\n",
        "\n",
        "¿Podría esta $B_2$ ser una mejor aproximación a $A$?\n",
        "\n",
        "$$\n",
        "B_2 =\n",
        "\\begin{bmatrix}\n",
        "3.5 & 3.5 &   &   \\\\\n",
        "3.5 & 3.5 &   &   \\\\\n",
        "    &     & 1.5 & 1.5 \\\\\n",
        "    &     & 1.5 & 1.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Los errores $A - B_2$ son solo 0.5 en la diagonal principal, mientras que $A - A_2$ tiene errores 2 y 1. Pero los errores fuera de la diagonal en $B_2$ (3.5 y 1.5) son demasiado grandes.\n",
        "\n",
        "¿Hay otra opción mejor?\n",
        "\n",
        "**No, $A_2$ es la mejor con rango $k = 2$**. Esto se prueba con la norma $L^2$ y luego con la de Frobenius.\n",
        "\n",
        "---\n",
        "\n",
        "### Teorema de Eckart-Young para $L^2$\n",
        "\n",
        "> **Eckart-Young en $L^2$**  \n",
        "> Si $\\operatorname{rank}(B) \\leq k$, entonces\n",
        "> $$\n",
        "> \\|A - B\\| = \\max \\frac{\\|(A - B)x\\|}{\\|x\\|} \\geq \\sigma_{k+1}.\n",
        "> \\tag{8}\n",
        "> $$\n"
      ],
      "metadata": {
        "id": "RLOuHHIBlXOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details><summary> Prueba:</summary>\n",
        "Sabemos que:\n",
        "$$\n",
        "\\|A - A_k\\| = \\sigma_{k+1}.\n",
        "$$\n",
        "La demostración completa de:\n",
        "$$\n",
        "\\|A - B\\| \\geq \\sigma_{k+1}\n",
        "$$\n",
        "depende de una buena elección del vector $x$ al calcular la norma $\\|A - B\\|$:\n",
        "\n",
        "Elegimos $x \\ne 0$ tal que:\n",
        "\n",
        "$$\n",
        "Bx = 0 \\quad \\text{y} \\quad x = \\sum_{i=1}^{k+1} c_i v_i \\tag{9}\n",
        "$$\n",
        "\n",
        "1. El núcleo de $B$ tiene dimensión $\\geq n - k$ porque $\\operatorname{rank}(B) \\leq k$.\n",
        "2. Las combinaciones lineales de $v_1$ a $v_{k+1}$ generan un subespacio de dimensión $k+1$.\n",
        "\n",
        "Estos dos subespacios **deben intersectarse**. Cuando las dimensiones suman más que $n$, comparten al menos una dirección. Por ejemplo, en $\\mathbb{R}^3$, dos planos se intersectan en una recta.\n",
        "\n",
        "Entonces, **elegimos un vector no nulo $x$ sobre esta recta**.\n",
        "\n",
        "---\n",
        "\n",
        "Usamos ese $x$ para estimar la norma de $A - B$ en (8). Recordemos que $Bx = 0$ y $Av_i = \\sigma_i u_i$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\|(A - B)x\\|^2 &= \\|Ax\\|^2 = \\left\\| \\sum c_i \\sigma_i u_i \\right\\|^2 \\\\\n",
        "&= \\sum_{i=1}^{k+1} c_i^2 \\sigma_i^2 \\tag{10}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Esta suma es al menos tan grande como:\n",
        "\n",
        "$$\n",
        "\\left(\\sum c_i^2\\right) \\sigma_{k+1}^2,\n",
        "$$\n",
        "\n",
        "que es exactamente $\\|x\\|^2 \\sigma_{k+1}^2$.\n",
        "\n",
        "---\n",
        "\n",
        "La ecuación (10) prueba que:\n",
        "\n",
        "$$\n",
        "\\|(A - B)x\\| \\geq \\sigma_{k+1} \\|x\\|.\n",
        "$$\n",
        "\n",
        "Este $x$ nos da la cota inferior deseada para $\\|A - B\\|$:\n",
        "\n",
        "$$\n",
        "\\frac{\\|(A - B)x\\|}{\\|x\\|} \\geq \\sigma_{k+1} \\quad \\Rightarrow \\quad \\|A - B\\| \\geq \\sigma_{k+1} = \\|A - A_k\\|. \\quad \\textbf{¡Demostrado!} \\tag{11}\n",
        "$$\n",
        "</details>"
      ],
      "metadata": {
        "id": "TGE2_oCYmJVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Pregunta**: ¿Qué es la transformación de Karhunen-Loève y cuál es su relación con la SVD?  \n",
        "  <details>\n",
        "  <summary>Respuesta</summary>\n",
        "  La transformación KL comienza con una matriz de covarianza $V$ de un proceso aleatorio de media cero.  \n",
        "  $V$ es simétrica y definida (semi)positiva. En general, $V$ puede ser una matriz infinita o una función de covarianza.  \n",
        "  Entonces, la expansión KL será una serie infinita.\n",
        "\n",
        "  Los vectores propios de $V$, ordenados según sus valores propios decrecientes $\\sigma_1^2 \\geq \\sigma_2^2 \\geq \\dots \\geq 0$,  \n",
        "  son las funciones base $u_i$ para la transformación KL. La expansión de cualquier vector $v$ en esta base ortonormal es:\n",
        "  $$\n",
        "  v = \\sum (u_i^T v) u_i\n",
        "  $$\n",
        "\n",
        "  En este caso estocástico, la transformación KL \"descorrelaciona\" el proceso aleatorio: los $u_i$ son independientes.  \n",
        "  Además, el ordenamiento de los valores propios implica que truncar en los primeros $k$ términos minimiza el error cuadrático esperado.  \n",
        "  Este hecho corresponde al teorema de Eckart-Young que se ve en la siguiente sección (I.9).\n",
        "\n",
        "  > La transformación KL es una forma estocástica (aleatoria) del Análisis de Componentes Principales (PCA).\n",
        "  </details>"
      ],
      "metadata": {
        "id": "HdxYT_y8tHyh"
      }
    }
  ]
}